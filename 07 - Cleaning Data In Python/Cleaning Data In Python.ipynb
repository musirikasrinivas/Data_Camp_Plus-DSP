{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data in python\n",
    "## Chapter-Exploring your data\n",
    "### Exercise 1\n",
    "### Loading and viewing your data\n",
    "In this chapter, you'll are going to look at a subset of the Department of Buildings Job Application Filings datasetfrom the [NYC Open Data](http://opendata.cityofnewyork.us/) portal. This dataset consists of job applications filed on January 22, 2017.\n",
    "Your first task is to load this dataset into a DataFrame and then inspect it using the `` .head() `` and `` .tail() `` methods. However, you&amp;apos;ll find out very quickly that the printed results don&amp;apos;t allow you to see everything you need, since there are too many columns.Therefore, you need to look at the data in another way. \n",
    "The `` .shape `` and `` .columns `` attributes let you see the shape of the DataFrame and obtain a list of its columns. From here, you can see which columns are relevant to the questions you&amp;apos;d like to ask of the data. To this end, a new DataFrame, `` df_subset ``, consisting only of these relevant columns, has been pre-loaded. This is the DataFrame you&amp;apos;ll work with in the rest of the chapter.\n",
    "Get acquainted with the dataset now by exploring it with pandas! This initial exploratory analysis is a crucial first step of data cleaning.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Import `` pandas `` as `` pd ``.\n",
    "*   Read ``dob_job_application_filings_subset.csv`` into a DataFrame called `` df ``.\n",
    "*   Print the head and tail of `` df ``.\n",
    "*   Print the shape of `` df `` and its columns. Note: `` .shape `` and `` .columns `` are _attributes_, not _methods_, so you don&amp;apos;t need to follow these with parentheses `` () ``.\n",
    "*   Hit &amp;apos;Submit Answer&amp;apos; to view the results! Notice the suspicious number of `` 0 `` values. Perhaps these represent missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siri\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Job #  Doc #        Borough House #                       Street Name  \\\n",
      "0  121577873      2      MANHATTAN     386  PARK AVENUE SOUTH                  \n",
      "1  520129502      1  STATEN ISLAND     107  KNOX PLACE                         \n",
      "2  121601560      1      MANHATTAN      63  WEST 131 STREET                    \n",
      "3  121601203      1      MANHATTAN      48  WEST 25TH STREET                   \n",
      "4  121601338      1      MANHATTAN      45  WEST 29 STREET                     \n",
      "\n",
      "   Block  Lot    Bin # Job Type Job Status       ...        \\\n",
      "0    857   38  1016890       A2          D       ...         \n",
      "1    342    1  5161350       A3          A       ...         \n",
      "2   1729    9  1053831       A2          Q       ...         \n",
      "3    826   69  1015610       A2          D       ...         \n",
      "4    831    7  1015754       A3          D       ...         \n",
      "\n",
      "                Owner's Last Name             Owner's Business Name  \\\n",
      "0  MIGLIORE                        MACKLOWE MANAGEMENT                \n",
      "1  BLUMENBERG                      NA                                 \n",
      "2  MARKOWITZ                       635 RIVERSIDE DRIVE NY LLC         \n",
      "3  CASALE                          48 W 25 ST LLC C/O BERNSTEIN       \n",
      "4  LEE                             HYUNG-HYANG REALTY CORP            \n",
      "\n",
      "  Owner's House Number          Owner'sHouse Street Name            City   \\\n",
      "0                  126  EAST 56TH STREET                  NEW YORK          \n",
      "1                  107  KNOX PLACE                        STATEN ISLAND     \n",
      "2                  619  WEST 54TH STREET                  NEW YORK          \n",
      "3                  150  WEST 30TH STREET                  NEW YORK          \n",
      "4                  614  8 AVENUE                          NEW YORK          \n",
      "\n",
      "  State    Zip Owner'sPhone #  \\\n",
      "0    NY  10222     2125545837   \n",
      "1    NY  10314     3477398892   \n",
      "2    NY  10016     2127652555   \n",
      "3    NY  10001     2125941414   \n",
      "4    NY  10001     2019881222   \n",
      "\n",
      "                                     Job Description      DOBRunDate  \n",
      "0  GENERAL MECHANICAL & PLUMBING MODIFICATIONS AS...  4/26/2013 0:00  \n",
      "1  BUILDERS PAVEMENT PLAN 143 LF.                ...  4/26/2013 0:00  \n",
      "2  GENERAL CONSTRUCTION TO INCLUDE NEW PARTITIONS...  4/26/2013 0:00  \n",
      "3  STRUCTURAL CHANGES ON THE 5TH FLOOR (MOONDOG E...  4/26/2013 0:00  \n",
      "4  FILING HEREWITH FACADE REPAIR PLANS. WORK SCOP...  4/26/2013 0:00  \n",
      "\n",
      "[5 rows x 84 columns]\n",
      "           Job #  Doc #        Borough House #  \\\n",
      "12841  520143988      1  STATEN ISLAND       8   \n",
      "12842  121613833      1      MANHATTAN     724   \n",
      "12843  121681260      1      MANHATTAN     350   \n",
      "12844  320771704      1       BROOKLYN     499   \n",
      "12845  520143951      1  STATEN ISLAND    1755   \n",
      "\n",
      "                            Street Name  Block  Lot    Bin # Job Type  \\\n",
      "12841  NOEL STREET                        5382   20  5069722       A2   \n",
      "12842  10 AVENUE                          1059    4  1082503       A2   \n",
      "12843  MANHATTAN AVE.                     1848   31  1055849       A2   \n",
      "12844  UNION STREET                        431   43  3007185       A2   \n",
      "12845  RICHMOND ROAD                       887   28  5022931       A2   \n",
      "\n",
      "      Job Status       ...                     Owner's Last Name  \\\n",
      "12841          D       ...        MALITO                           \n",
      "12842          D       ...        CROMAN                           \n",
      "12843          A       ...        ARYEH                            \n",
      "12844          D       ...        WIGGINS                          \n",
      "12845          D       ...        CAMBRIA                          \n",
      "\n",
      "                  Owner's Business Name Owner's House Number  \\\n",
      "12841  GENO MALITO                                         8   \n",
      "12842  722-724 10TH AVENUE HOLDING LLC                   632   \n",
      "12843  DG UWS LLC                                        619   \n",
      "12844  N/A                                                77   \n",
      "12845  RONALD CAMBRIA                                   1755   \n",
      "\n",
      "               Owner'sHouse Street Name            City  State    Zip  \\\n",
      "12841  NOEL STREET                       STATEN ISLAND      NY  10312   \n",
      "12842  BROADWAY                          NEW YORK           NY  10012   \n",
      "12843  WEST 54TH STREET                  NEW YORK           NY  10019   \n",
      "12844  PROSPECT PLACE                    BROOKLYN           NY  11217   \n",
      "12845  RICHMOND ROAD                     STATEN ISLAND      NY  10304   \n",
      "\n",
      "      Owner'sPhone #                                    Job Description  \\\n",
      "12841     9174685659  HORIZONTAL ENLARGEMENT OF ATTACHED ONE CAR GAR...   \n",
      "12842     2122289300  RENOVATION OF EXISTING APARTMENT #3B ON THIRD ...   \n",
      "12843     2127652555  REPLACE BURNER IN EXSTG BOILER WITH NEW GAS BU...   \n",
      "12844     9178487799  INSTALL NEW SPRINKLER SYSTEM THROUGHOUT THE BU...   \n",
      "12845     7184482740  INTERIOR PARTITIONS AND MINOR PLUMBING WORK TO...   \n",
      "\n",
      "           DOBRunDate  \n",
      "12841  6/13/2013 0:00  \n",
      "12842  6/13/2013 0:00  \n",
      "12843  6/13/2013 0:00  \n",
      "12844  6/13/2013 0:00  \n",
      "12845  6/13/2013 0:00  \n",
      "\n",
      "[5 rows x 84 columns]\n",
      "(12846, 84)\n",
      "Index(['Job #', 'Doc #', 'Borough', 'House #', 'Street Name', 'Block', 'Lot',\n",
      "       'Bin #', 'Job Type', 'Job Status', 'Job Status Descrp',\n",
      "       'Latest Action Date', 'Building Type', 'Community - Board', 'Cluster',\n",
      "       'Landmarked', 'Adult Estab', 'Loft Board', 'City Owned', 'Little e',\n",
      "       'PC Filed', 'eFiling Filed', 'Plumbing', 'Mechanical', 'Boiler',\n",
      "       'Fuel Burning', 'Fuel Storage', 'Standpipe', 'Sprinkler', 'Fire Alarm',\n",
      "       'Equipment', 'Fire Suppression', 'Curb Cut', 'Other',\n",
      "       'Other Description', 'Applicant's First Name', 'Applicant's Last Name',\n",
      "       'Applicant Professional Title', 'Applicant License #',\n",
      "       'Professional Cert', 'Pre- Filing Date', 'Paid', 'Fully Paid',\n",
      "       'Assigned', 'Approved', 'Fully Permitted', 'old _initial_cost',\n",
      "       'initial_cost', 'Total Est. Fee', 'total_est_fee', 'Fee Status',\n",
      "       'Existing Zoning Sqft', 'Proposed Zoning Sqft', 'Horizontal Enlrgmt',\n",
      "       'Vertical Enlrgmt', 'Enlargement SQ Footage', 'Street Frontage',\n",
      "       'ExistingNo. of Stories', 'Proposed No. of Stories', 'Existing Height',\n",
      "       'Proposed Height', 'Existing Dwelling Units', 'Proposed Dwelling Units',\n",
      "       'Existing Occupancy', 'Proposed Occupancy', 'Site Fill', 'Zoning Dist1',\n",
      "       'Zoning Dist2', 'Zoning Dist3', 'Special District 1',\n",
      "       'Special District 2', 'Owner Type', 'Non-Profit', 'Owner's First Name',\n",
      "       'Owner's Last Name', 'Owner's Business Name', 'Owner's House Number',\n",
      "       'Owner'sHouse Street Name', 'City ', 'State', 'Zip', 'Owner'sPhone #',\n",
      "       'Job Description', 'DOBRunDate'],\n",
      "      dtype='object')\n",
      "       Job #  Doc #        Borough House #                       Street Name  \\\n",
      "0  121577873      2      MANHATTAN     386  PARK AVENUE SOUTH                  \n",
      "1  520129502      1  STATEN ISLAND     107  KNOX PLACE                         \n",
      "2  121601560      1      MANHATTAN      63  WEST 131 STREET                    \n",
      "3  121601203      1      MANHATTAN      48  WEST 25TH STREET                   \n",
      "4  121601338      1      MANHATTAN      45  WEST 29 STREET                     \n",
      "\n",
      "   Block  Lot    Bin # Job Type Job Status       ...        \\\n",
      "0    857   38  1016890       A2          D       ...         \n",
      "1    342    1  5161350       A3          A       ...         \n",
      "2   1729    9  1053831       A2          Q       ...         \n",
      "3    826   69  1015610       A2          D       ...         \n",
      "4    831    7  1015754       A3          D       ...         \n",
      "\n",
      "                Owner's Last Name             Owner's Business Name  \\\n",
      "0  MIGLIORE                        MACKLOWE MANAGEMENT                \n",
      "1  BLUMENBERG                      NA                                 \n",
      "2  MARKOWITZ                       635 RIVERSIDE DRIVE NY LLC         \n",
      "3  CASALE                          48 W 25 ST LLC C/O BERNSTEIN       \n",
      "4  LEE                             HYUNG-HYANG REALTY CORP            \n",
      "\n",
      "  Owner's House Number          Owner'sHouse Street Name            City   \\\n",
      "0                  126  EAST 56TH STREET                  NEW YORK          \n",
      "1                  107  KNOX PLACE                        STATEN ISLAND     \n",
      "2                  619  WEST 54TH STREET                  NEW YORK          \n",
      "3                  150  WEST 30TH STREET                  NEW YORK          \n",
      "4                  614  8 AVENUE                          NEW YORK          \n",
      "\n",
      "  State    Zip Owner'sPhone #  \\\n",
      "0    NY  10222     2125545837   \n",
      "1    NY  10314     3477398892   \n",
      "2    NY  10016     2127652555   \n",
      "3    NY  10001     2125941414   \n",
      "4    NY  10001     2019881222   \n",
      "\n",
      "                                     Job Description      DOBRunDate  \n",
      "0  GENERAL MECHANICAL & PLUMBING MODIFICATIONS AS...  4/26/2013 0:00  \n",
      "1  BUILDERS PAVEMENT PLAN 143 LF.                ...  4/26/2013 0:00  \n",
      "2  GENERAL CONSTRUCTION TO INCLUDE NEW PARTITIONS...  4/26/2013 0:00  \n",
      "3  STRUCTURAL CHANGES ON THE 5TH FLOOR (MOONDOG E...  4/26/2013 0:00  \n",
      "4  FILING HEREWITH FACADE REPAIR PLANS. WORK SCOP...  4/26/2013 0:00  \n",
      "\n",
      "[5 rows x 84 columns]\n",
      "           Job #  Doc #        Borough House #  \\\n",
      "12841  520143988      1  STATEN ISLAND       8   \n",
      "12842  121613833      1      MANHATTAN     724   \n",
      "12843  121681260      1      MANHATTAN     350   \n",
      "12844  320771704      1       BROOKLYN     499   \n",
      "12845  520143951      1  STATEN ISLAND    1755   \n",
      "\n",
      "                            Street Name  Block  Lot    Bin # Job Type  \\\n",
      "12841  NOEL STREET                        5382   20  5069722       A2   \n",
      "12842  10 AVENUE                          1059    4  1082503       A2   \n",
      "12843  MANHATTAN AVE.                     1848   31  1055849       A2   \n",
      "12844  UNION STREET                        431   43  3007185       A2   \n",
      "12845  RICHMOND ROAD                       887   28  5022931       A2   \n",
      "\n",
      "      Job Status       ...                     Owner's Last Name  \\\n",
      "12841          D       ...        MALITO                           \n",
      "12842          D       ...        CROMAN                           \n",
      "12843          A       ...        ARYEH                            \n",
      "12844          D       ...        WIGGINS                          \n",
      "12845          D       ...        CAMBRIA                          \n",
      "\n",
      "                  Owner's Business Name Owner's House Number  \\\n",
      "12841  GENO MALITO                                         8   \n",
      "12842  722-724 10TH AVENUE HOLDING LLC                   632   \n",
      "12843  DG UWS LLC                                        619   \n",
      "12844  N/A                                                77   \n",
      "12845  RONALD CAMBRIA                                   1755   \n",
      "\n",
      "               Owner'sHouse Street Name            City  State    Zip  \\\n",
      "12841  NOEL STREET                       STATEN ISLAND      NY  10312   \n",
      "12842  BROADWAY                          NEW YORK           NY  10012   \n",
      "12843  WEST 54TH STREET                  NEW YORK           NY  10019   \n",
      "12844  PROSPECT PLACE                    BROOKLYN           NY  11217   \n",
      "12845  RICHMOND ROAD                     STATEN ISLAND      NY  10304   \n",
      "\n",
      "      Owner'sPhone #                                    Job Description  \\\n",
      "12841     9174685659  HORIZONTAL ENLARGEMENT OF ATTACHED ONE CAR GAR...   \n",
      "12842     2122289300  RENOVATION OF EXISTING APARTMENT #3B ON THIRD ...   \n",
      "12843     2127652555  REPLACE BURNER IN EXSTG BOILER WITH NEW GAS BU...   \n",
      "12844     9178487799  INSTALL NEW SPRINKLER SYSTEM THROUGHOUT THE BU...   \n",
      "12845     7184482740  INTERIOR PARTITIONS AND MINOR PLUMBING WORK TO...   \n",
      "\n",
      "           DOBRunDate  \n",
      "12841  6/13/2013 0:00  \n",
      "12842  6/13/2013 0:00  \n",
      "12843  6/13/2013 0:00  \n",
      "12844  6/13/2013 0:00  \n",
      "12845  6/13/2013 0:00  \n",
      "\n",
      "[5 rows x 84 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the file into a DataFrame: df\n",
    "df = pd.read_csv('dob_job_application_filings_subset.csv')\n",
    "df_subset = pd.read_csv('dob_job_application_filings_subset.csv')\n",
    "\n",
    "\n",
    "# Print the head of df\n",
    "print(df.head())\n",
    "\n",
    "# Print the tail of df\n",
    "print(df.tail())\n",
    "\n",
    "# Print the shape of df\n",
    "print(df.shape)\n",
    "\n",
    "# Print the columns of df\n",
    "print(df.columns)\n",
    "\n",
    "# Print the head and tail of df_subset\n",
    "print(df_subset.head())\n",
    "print(df_subset.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "### Further diagnosis\n",
    "In the previous exercise, you identified some potentially unclean or missing data. Now, you&amp;apos;ll continue to diagnose your data with the very useful `` .info() `` method. \n",
    "The `` .info() `` method provides important information about a DataFrame, such as the number of rows, number of columns, number of non-missing values in each column, and the data type stored in each column. This is the kind of information that will allow you to confirm whether the `` &amp;apos;Initial Cost&amp;apos; `` and `` &amp;apos;Total Est. Fee&amp;apos; `` columns are numeric or strings. From the results, you&amp;apos;ll also be able to see whether or not all columns have complete data in them. \n",
    "The full DataFrame `` df `` and the subset DataFrame `` df_subset `` have been pre-loaded. Your task is to use the `` .info() `` method on these and analyze the results.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Print the `` info `` of `` df ``.\n",
    "*   Print the `` info `` of the subset dataframe, `` df_subset ``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12846 entries, 0 to 12845\n",
      "Data columns (total 84 columns):\n",
      "Job #                           12846 non-null int64\n",
      "Doc #                           12846 non-null int64\n",
      "Borough                         12846 non-null object\n",
      "House #                         12846 non-null object\n",
      "Street Name                     12846 non-null object\n",
      "Block                           12846 non-null int64\n",
      "Lot                             12846 non-null int64\n",
      "Bin #                           12846 non-null int64\n",
      "Job Type                        12846 non-null object\n",
      "Job Status                      12846 non-null object\n",
      "Job Status Descrp               12846 non-null object\n",
      "Latest Action Date              12846 non-null object\n",
      "Building Type                   12846 non-null object\n",
      "Community - Board               12846 non-null object\n",
      "Cluster                         0 non-null float64\n",
      "Landmarked                      2067 non-null object\n",
      "Adult Estab                     1 non-null object\n",
      "Loft Board                      65 non-null object\n",
      "City Owned                      1419 non-null object\n",
      "Little e                        365 non-null object\n",
      "PC Filed                        0 non-null float64\n",
      "eFiling Filed                   12846 non-null object\n",
      "Plumbing                        12846 non-null object\n",
      "Mechanical                      12846 non-null object\n",
      "Boiler                          12846 non-null object\n",
      "Fuel Burning                    12846 non-null object\n",
      "Fuel Storage                    12846 non-null object\n",
      "Standpipe                       12846 non-null object\n",
      "Sprinkler                       12846 non-null object\n",
      "Fire Alarm                      12846 non-null object\n",
      "Equipment                       12846 non-null object\n",
      "Fire Suppression                12846 non-null object\n",
      "Curb Cut                        12846 non-null object\n",
      "Other                           12846 non-null object\n",
      "Other Description               12846 non-null object\n",
      "Applicant's First Name          12846 non-null object\n",
      "Applicant's Last Name           12846 non-null object\n",
      "Applicant Professional Title    12846 non-null object\n",
      "Applicant License #             12846 non-null object\n",
      "Professional Cert               6908 non-null object\n",
      "Pre- Filing Date                12846 non-null object\n",
      "Paid                            11961 non-null object\n",
      "Fully Paid                      11963 non-null object\n",
      "Assigned                        3817 non-null object\n",
      "Approved                        4062 non-null object\n",
      "Fully Permitted                 1495 non-null object\n",
      "old _initial_cost               12846 non-null object\n",
      "initial_cost                    12846 non-null int64\n",
      "Total Est. Fee                  12846 non-null object\n",
      "total_est_fee                   12846 non-null float64\n",
      "Fee Status                      12846 non-null object\n",
      "Existing Zoning Sqft            12846 non-null int64\n",
      "Proposed Zoning Sqft            12846 non-null int64\n",
      "Horizontal Enlrgmt              231 non-null object\n",
      "Vertical Enlrgmt                142 non-null object\n",
      "Enlargement SQ Footage          12846 non-null int64\n",
      "Street Frontage                 12846 non-null int64\n",
      "ExistingNo. of Stories          12846 non-null int64\n",
      "Proposed No. of Stories         12846 non-null int64\n",
      "Existing Height                 12846 non-null int64\n",
      "Proposed Height                 12846 non-null int64\n",
      "Existing Dwelling Units         12846 non-null object\n",
      "Proposed Dwelling Units         12846 non-null object\n",
      "Existing Occupancy              12846 non-null object\n",
      "Proposed Occupancy              12846 non-null object\n",
      "Site Fill                       8641 non-null object\n",
      "Zoning Dist1                    11263 non-null object\n",
      "Zoning Dist2                    1652 non-null object\n",
      "Zoning Dist3                    88 non-null object\n",
      "Special District 1              3062 non-null object\n",
      "Special District 2              848 non-null object\n",
      "Owner Type                      0 non-null float64\n",
      "Non-Profit                      971 non-null object\n",
      "Owner's First Name              12846 non-null object\n",
      "Owner's Last Name               12846 non-null object\n",
      "Owner's Business Name           12846 non-null object\n",
      "Owner's House Number            12846 non-null object\n",
      "Owner'sHouse Street Name        12846 non-null object\n",
      "City                            12846 non-null object\n",
      "State                           12846 non-null object\n",
      "Zip                             12846 non-null int64\n",
      "Owner'sPhone #                  12846 non-null int64\n",
      "Job Description                 12699 non-null object\n",
      "DOBRunDate                      12846 non-null object\n",
      "dtypes: float64(4), int64(16), object(64)\n",
      "memory usage: 8.2+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12846 entries, 0 to 12845\n",
      "Data columns (total 84 columns):\n",
      "Job #                           12846 non-null int64\n",
      "Doc #                           12846 non-null int64\n",
      "Borough                         12846 non-null object\n",
      "House #                         12846 non-null object\n",
      "Street Name                     12846 non-null object\n",
      "Block                           12846 non-null int64\n",
      "Lot                             12846 non-null int64\n",
      "Bin #                           12846 non-null int64\n",
      "Job Type                        12846 non-null object\n",
      "Job Status                      12846 non-null object\n",
      "Job Status Descrp               12846 non-null object\n",
      "Latest Action Date              12846 non-null object\n",
      "Building Type                   12846 non-null object\n",
      "Community - Board               12846 non-null object\n",
      "Cluster                         0 non-null float64\n",
      "Landmarked                      2067 non-null object\n",
      "Adult Estab                     1 non-null object\n",
      "Loft Board                      65 non-null object\n",
      "City Owned                      1419 non-null object\n",
      "Little e                        365 non-null object\n",
      "PC Filed                        0 non-null float64\n",
      "eFiling Filed                   12846 non-null object\n",
      "Plumbing                        12846 non-null object\n",
      "Mechanical                      12846 non-null object\n",
      "Boiler                          12846 non-null object\n",
      "Fuel Burning                    12846 non-null object\n",
      "Fuel Storage                    12846 non-null object\n",
      "Standpipe                       12846 non-null object\n",
      "Sprinkler                       12846 non-null object\n",
      "Fire Alarm                      12846 non-null object\n",
      "Equipment                       12846 non-null object\n",
      "Fire Suppression                12846 non-null object\n",
      "Curb Cut                        12846 non-null object\n",
      "Other                           12846 non-null object\n",
      "Other Description               12846 non-null object\n",
      "Applicant's First Name          12846 non-null object\n",
      "Applicant's Last Name           12846 non-null object\n",
      "Applicant Professional Title    12846 non-null object\n",
      "Applicant License #             12846 non-null object\n",
      "Professional Cert               6908 non-null object\n",
      "Pre- Filing Date                12846 non-null object\n",
      "Paid                            11961 non-null object\n",
      "Fully Paid                      11963 non-null object\n",
      "Assigned                        3817 non-null object\n",
      "Approved                        4062 non-null object\n",
      "Fully Permitted                 1495 non-null object\n",
      "old _initial_cost               12846 non-null object\n",
      "initial_cost                    12846 non-null int64\n",
      "Total Est. Fee                  12846 non-null object\n",
      "total_est_fee                   12846 non-null float64\n",
      "Fee Status                      12846 non-null object\n",
      "Existing Zoning Sqft            12846 non-null int64\n",
      "Proposed Zoning Sqft            12846 non-null int64\n",
      "Horizontal Enlrgmt              231 non-null object\n",
      "Vertical Enlrgmt                142 non-null object\n",
      "Enlargement SQ Footage          12846 non-null int64\n",
      "Street Frontage                 12846 non-null int64\n",
      "ExistingNo. of Stories          12846 non-null int64\n",
      "Proposed No. of Stories         12846 non-null int64\n",
      "Existing Height                 12846 non-null int64\n",
      "Proposed Height                 12846 non-null int64\n",
      "Existing Dwelling Units         12846 non-null object\n",
      "Proposed Dwelling Units         12846 non-null object\n",
      "Existing Occupancy              12846 non-null object\n",
      "Proposed Occupancy              12846 non-null object\n",
      "Site Fill                       8641 non-null object\n",
      "Zoning Dist1                    11263 non-null object\n",
      "Zoning Dist2                    1652 non-null object\n",
      "Zoning Dist3                    88 non-null object\n",
      "Special District 1              3062 non-null object\n",
      "Special District 2              848 non-null object\n",
      "Owner Type                      0 non-null float64\n",
      "Non-Profit                      971 non-null object\n",
      "Owner's First Name              12846 non-null object\n",
      "Owner's Last Name               12846 non-null object\n",
      "Owner's Business Name           12846 non-null object\n",
      "Owner's House Number            12846 non-null object\n",
      "Owner'sHouse Street Name        12846 non-null object\n",
      "City                            12846 non-null object\n",
      "State                           12846 non-null object\n",
      "Zip                             12846 non-null int64\n",
      "Owner'sPhone #                  12846 non-null int64\n",
      "Job Description                 12699 non-null object\n",
      "DOBRunDate                      12846 non-null object\n",
      "dtypes: float64(4), int64(16), object(64)\n",
      "memory usage: 8.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print the info of df\n",
    "print(df.info())\n",
    "\n",
    "# Print the info of df_subset\n",
    "print(df_subset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 4\n",
    "\n",
    "### Calculating summary statistics\n",
    "\n",
    "You'll now use the `.describe()` method to calculate summary statistics of your data.\n",
    "\n",
    "In this exercise, the columns `'Initial Cost'` and `'Total Est. Fee'` have been cleaned up for you. That is, the dollar sign has been removed and they have been converted into two new numeric columns: `initial_cost` and `total_est_fee`. You'll learn how to do this yourself in later chapters. It's also worth noting that some columns such as `Job #` are encoded as numeric columns, but it does not make sense to compute summary statistics for such columns.\n",
    "\n",
    "This cleaned DataFrame has been pre-loaded as `df`. Your job is to use the `.describe()` method on it in the IPython Shell and select the statement below that is False.\n",
    "\n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1.The mean of 'Proposed No. of Stories' is 8.144325.\n",
    "\n",
    "2.The standard deviation of 'Existing Height' is 146.917360.\n",
    "\n",
    "3.There are 12846 entries in the DataFrame.\n",
    "\n",
    "4.The standard deviation of 'Street Frontage' is 11.874080.\n",
    "\n",
    "5.The maximum of 'Proposed Height' is 4200.\n",
    "\n",
    "\n",
    "#### Answer-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "### Frequency counts for categorical data\n",
    "As you&amp;apos;ve seen, `` .describe() `` can only be used on numeric columns. So how can you diagnose data issues when you have categorical data? One way is by using the `` .value_counts() `` method, which returns the frequency counts for each unique value in a column!\n",
    "This method also has an optional parameter called `` dropna `` which is `` True `` by default.What this means is if you have missing data in a column, it will not give a frequency count of them.You want to set the `` dropna `` column to `` False `` so if there are missing values in a column, it will give you the frequency counts.\n",
    "In this exercise, you&amp;apos;re going to look at the `` &amp;apos;Borough&amp;apos; ``, `` &amp;apos;State&amp;apos; ``, and `` &amp;apos;Site Fill&amp;apos; `` columns to make sure all the values in there are valid. When looking at the output, do a sanity check: Are all values in the `` &amp;apos;State&amp;apos; `` column from `` NY ``, for example? Since the dataset consists of applications filed in NY, you would expect this to be the case.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Print the value counts for:\n",
    "    \n",
    "    *   The `` &amp;apos;Borough&amp;apos; `` column.\n",
    "    *   The `` &amp;apos;State&amp;apos; `` column.\n",
    "    *   The `` &amp;apos;Site Fill&amp;apos; `` column.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MANHATTAN        6310\n",
      "BROOKLYN         2866\n",
      "QUEENS           2121\n",
      "BRONX             974\n",
      "STATEN ISLAND     575\n",
      "Name: Borough, dtype: int64\n",
      "NY    12391\n",
      "NJ      241\n",
      "PA       38\n",
      "CA       20\n",
      "OH       19\n",
      "IL       17\n",
      "FL       17\n",
      "CT       16\n",
      "TX       13\n",
      "TN       10\n",
      "MD        7\n",
      "DC        7\n",
      "GA        6\n",
      "KS        6\n",
      "MA        6\n",
      "VA        5\n",
      "CO        4\n",
      "SC        3\n",
      "WI        3\n",
      "MN        3\n",
      "AZ        3\n",
      "UT        2\n",
      "NC        2\n",
      "RI        2\n",
      "WA        1\n",
      "NM        1\n",
      "VT        1\n",
      "IN        1\n",
      "MI        1\n",
      "Name: State, dtype: int64\n",
      "NOT APPLICABLE                              7806\n",
      "NaN                                         4205\n",
      "ON-SITE                                      519\n",
      "OFF-SITE                                     186\n",
      "USE UNDER 300 CU.YD                          130\n",
      "Name: Site Fill, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the value counts for 'Borough'\n",
    "print(df['Borough'].value_counts(dropna=False))\n",
    "\n",
    "# Print the value_counts for 'State'\n",
    "print(df['State'].value_counts(dropna=False))\n",
    "\n",
    "# Print the value counts for 'Site Fill'\n",
    "print(df['Site Fill'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "### Visualizing single variables with histograms\n",
    "Up until now, you&amp;apos;ve been looking at descriptive statistics of your data. One of the best ways to confirm what the numbers are telling you is to plot and visualize the data.\n",
    "You&amp;apos;ll start by visualizing single variables using a histogram for numeric values. The column you will work on in this exercise is `` Existing Zoning Sqft ``.\n",
    "The `` .plot() `` method allows you to create a plot of each column of a DataFrame. The `` kind `` parameter allows you to specify the type of plot to use - `` kind=hist ``, for example, plots a histogram.\n",
    "In the IPython Shell, begin by computing summary statistics for the `` Existing Zoning Sqft `` column using the `` .describe() `` method. You&amp;apos;ll notice that there are extremely large differences between the `` min `` and `` max `` values,and the plot will need to be adjusted accordingly. In such cases, it&amp;apos;s good to look at the plot on a log scale. The keyword arguments `` logx=True `` or `` logy=True `` can be passed in to `` .plot() `` depending on which axis you want to rescale.\n",
    "Finally, note that Python will render a plot such that the axis will hold all the information. That is, if you end up with large amounts of whitespace in your plot, it indicates counts or values too small to render.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Import `` matplotlib.pyplot `` as `` plt ``.\n",
    "*   Create a histogram of the `` Existing Zoning Sqft `` column. Rotate the axis labels by 70 degrees and use a log scale for both axes.\n",
    "*   Display the histogram using `` plt.show() ``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the histogram\n",
    "df['Existing Zoning Sqft'].plot(kind='hist', rot=70, logx=True, logy=True)\n",
    "\n",
    "# Display the histogram\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "### Visualizing multiple variables with boxplots\n",
    "Histograms are great ways of visualizing single variables. To visualize multiple variables, boxplots are useful, especially when one of the variables is categorical. \n",
    "In this exercise, your job is to use a boxplot to compare the `` initial_cost `` across the different values of the `` Borough `` column. The pandas `` .boxplot() `` method is a quick way to do this, in which you have to specify the `` column `` and `` by `` parameters. Here, you want to visualize how `` initial_cost `` varies _by_ `` Borough ``.\n",
    "`` pandas `` and `` matplotlib.pyplot `` have been imported for you as `` pd `` and `` plt ``, respectively, and the DataFrame has been pre-loaded as `` df ``.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Using the `` .boxplot() `` method of `` df ``, create a boxplot of `` initial_cost `` across the different values of `` Borough ``.\n",
    "*   Display the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siri\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x21ada851940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('dob_job_application_filings_subset.csv')\n",
    "\n",
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the boxplot\n",
    "df.boxplot(column='initial_cost', by='Borough', rot=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "\n",
    "### Visualizing multiple variables with scatter plots\n",
    "\n",
    "Boxplots are great when you have a numeric column that you want to compare across different categories. When you want to visualize two numeric columns, scatter plots are ideal.\n",
    "In this exercise, your job is to make a scatter plot with `` initial_cost `` on the x-axis and the `` total_est_fee`` on the y-axis. You can do this by using the DataFrame `` .plot() `` method with `` kind=scatter``.You&amp;apos;ll notice right away that there are 2 major outliers shown in the plots.\n",
    "Since these outliers dominate the plot, an additional DataFrame, `` df_subset ``, has been provided, in which some of the extreme values have been removed. After making a scatter plot using this, you'llfind some interesting patterns here that would not have been seen by looking at summary statistics or 1 variable plots.\n",
    "When you are done, you can cycle between the two plots by clicking the Previous Plot and Next Plot buttons below the plot.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Using `` df ``, create a scatter plot (`` kind=scatter ``) with `` initial_cost `` on the x-axis and the `` total_est_fee`` on the y-axis. Rotate the x-axis labels by 70 degrees.\n",
    "*   Create another scatter plot exactly as above, substituting `` df_subset `` in place of `` df ``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.read_csv('dob_job_application_filings_subset.csv')\n",
    "\n",
    "# Create and display the first scatter plot\n",
    "df.plot(kind='scatter', x='initial_cost', y='total_est_fee', rot=70)\n",
    "plt.show()\n",
    "\n",
    "df_subset = df[9107:10818] \n",
    "df_subset = df_subset[df_subset['total_est_fee'] <  19014.5 ]\n",
    "df_subset = df_subset[df_subset['initial_cost'] < 1020211.0 ]\n",
    "\n",
    "# plt.xlim(left= -48581.504959021724, right=1020211.5049590217)\n",
    "# plt.ylim(-905.460654964219, 19014.54065496422)\n",
    "\n",
    "# Create and display the second scatter plot\n",
    "df_subset.plot(kind='scatter', x='initial_cost', y='total_est_fee', rot=70,\n",
    "               xlim= (-48581.504959021724, 1020211.5049590217), ylim = (-905.460654964219, 19014.54065496422) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter-Tidying data for analysis\n",
    "\n",
    "### Excercise-1\n",
    "\n",
    "For data to be tidy, it must have:\n",
    "\n",
    "* Each variable as a separate column.\n",
    "* Each row as a separate observation.\n",
    "\n",
    "As a data scientist, you'll encounter data that is represented in a variety of different ways, so it is important to be able to recognize tidy (or untidy) data when you see it.\n",
    "\n",
    "In this exercise, two example datasets have been pre-loaded into the DataFrames `df1` and `df2`. Only one of them is tidy. Your job is to explore these further in the IPython Shell and identify the one that is not tidy, and why it is not tidy.\n",
    "\n",
    "In the rest of this course, you will frequently be asked to explore the structure of DataFrames in the IPython Shell prior to performing different operations on them. Doing this will not only strengthen your comprehension of the data cleaning concepts covered in this course, but will also help you realize and take advantage of the relationship between working in the Shell and in the script.\n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1.`df2;` the rows are not all separate observations.\n",
    "\n",
    "2.`df1;` each variable is not a separate column.\n",
    "\n",
    "3.`df2;` each variable is not a separate column.\n",
    "\n",
    "4.`df1;` the rows are not all separate observations.\n",
    "\n",
    "\n",
    "#### Answer-3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "### Reshaping your data using melt\n",
    "Melting data is the process of turning columns of your data into rows of data. Consider the DataFrames from the previous exercise. In the tidy DataFrame, the variables `` Ozone ``, `` Solar.R ``, `` Wind ``, and `` Temp `` each had their own column.If, however, you wanted these variables to be in rows instead, you could melt the DataFrame. In doing so, however, you would make the data untidy! This is important to keep in mind: Depending on how your data is represented, you will have to reshape itdifferently (e.g., this could make it easier to plot values).\n",
    "In this exercise, you will practice melting a DataFrame using `` pd.melt() ``. There are two parameters you should be aware of: `` id_vars `` and `` value_vars ``.The `` id_vars `` represent the columns of the data you __do not__ want to melt (i.e., keep it in its current shape), while the `` value_vars `` represent the columns you __do__ wish to melt into rows. By default, if no `` value_vars `` are provided, all columns not set in the `` id_vars `` will be melted. This could save a bit of typing, depending on the number of columns that need to be melted.\n",
    "The (tidy) DataFrame `` airquality `` has been pre-loaded. Your job is to melt its `` Ozone ``, `` Solar.R ``, `` Wind ``, and `` Temp `` columns into rows. Later in this chapter, you&amp;apos;ll learn how to bring this melted DataFrame back into a tidy form.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Print the head of `` airquality ``.\n",
    "*   Use `` pd.melt() `` to melt the `` Ozone ``, `` Solar.R ``, `` Wind ``, and `` Temp `` columns of `` airquality `` into rows. Do this by using `` id_vars `` to specify the columns you __do not__ wish to melt: `` &amp;apos;Month&amp;apos; `` and `` &amp;apos;Day&amp;apos; ``.\n",
    "*   Print the head of `` airquality_melt ``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ozone  Solar.R  Wind  Temp  Month  Day\n",
      "0   41.0    190.0   7.4    67      5    1\n",
      "1   36.0    118.0   8.0    72      5    2\n",
      "2   12.0    149.0  12.6    74      5    3\n",
      "3   18.0    313.0  11.5    62      5    4\n",
      "4    NaN      NaN  14.3    56      5    5\n",
      "   Month  Day variable  value\n",
      "0      5    1    Ozone   41.0\n",
      "1      5    2    Ozone   36.0\n",
      "2      5    3    Ozone   12.0\n",
      "3      5    4    Ozone   18.0\n",
      "4      5    5    Ozone    NaN\n"
     ]
    }
   ],
   "source": [
    "\n",
    "airquality=pd.read_csv('airquality.csv')\n",
    "\n",
    "# Print the head of airquality\n",
    "print(airquality.head())\n",
    "\n",
    "# Melt airquality: airquality_melt\n",
    "airquality_melt = pd.melt(frame=airquality, id_vars=['Month', 'Day'])\n",
    "\n",
    "# Print the head of airquality_melt\n",
    "print(airquality_melt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise 3\n",
    "### Customizing melted data\n",
    "When melting DataFrames, it would be better to have column names more meaningful than `` variable `` and `` value `` (the default names used by `` pd.melt() ``).\n",
    "The default names may work in certain situations, but it&amp;apos;s best to always have data that is self explanatory.\n",
    "You can rename the `` variable `` column by specifying an argument to the `` var_name `` parameter, and the `` value ``column by specifying an argument to the `` value_name `` parameter. You will now practice doing exactly this.Pandas as `` pd `` and the DataFrame `` airquality `` has been pre-loaded for you.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Print the head of `` airquality ``.\n",
    "*   Melt the columns of `` airquality with the default ``variable`` column renamed to ``&amp;apos;measurement&amp;apos;`` and the default ``value`` column renamed to ``&amp;apos;reading&amp;apos;`` . You can do this by specifying, respectively, the ``var_name`` and ``value_name\\` parameters.\n",
    "*   Print the head of `` airquality_melt ``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ozone  Solar.R  Wind  Temp  Month  Day\n",
      "0   41.0    190.0   7.4    67      5    1\n",
      "1   36.0    118.0   8.0    72      5    2\n",
      "2   12.0    149.0  12.6    74      5    3\n",
      "3   18.0    313.0  11.5    62      5    4\n",
      "4    NaN      NaN  14.3    56      5    5\n",
      "   Month  Day measurement  reading\n",
      "0      5    1       Ozone     41.0\n",
      "1      5    2       Ozone     36.0\n",
      "2      5    3       Ozone     12.0\n",
      "3      5    4       Ozone     18.0\n",
      "4      5    5       Ozone      NaN\n"
     ]
    }
   ],
   "source": [
    "# Print the head of airquality\n",
    "print(airquality.head())\n",
    "\n",
    "# Melt airquality: airquality_melt\n",
    "airquality_melt = pd.melt(frame=airquality, id_vars=['Month', 'Day'], var_name='measurement', value_name='reading')\n",
    "\n",
    "# Print the head of airquality_melt\n",
    "print(airquality_melt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "### Pivot data\n",
    "Pivoting data is the opposite of melting it. Remember the tidy form that the `` airquality `` DataFrame was in before you melted it? You&amp;apos;ll now begin pivoting it back into that form using the `` .pivot_table() `` method!\n",
    "While melting takes a set of columns and turns it into a single column, pivoting will create a new column for each unique value in a specified column.\n",
    "`` .pivot_table() `` has an `` index `` parameter which you can use to specify the columns that you _don&amp;apos;t_ want pivoted: It is similar to the `` id_vars `` parameter of `` pd.melt() ``. Two other parameters that you have to specify are `` columns `` (the name of the column you want to pivot), and `` values `` (the values to be used when the column is pivoted). The melted DataFrame `` airquality_melt `` has been pre-loaded for you.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Print the head of `` airquality_melt ``.\n",
    "*   Pivot `` airquality_melt `` by using `` .pivot_table() `` with the rows indexed by `` &amp;apos;Month&amp;apos; `` and `` &amp;apos;Day&amp;apos; ``, the columns indexed by `` &amp;apos;measurement&amp;apos; ``, and the values populated with `` &amp;apos;reading&amp;apos; ``.\n",
    "*   Print the head of `` airquality_pivot ``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Month  Day measurement  reading\n",
      "0      5    1       Ozone     41.0\n",
      "1      5    2       Ozone     36.0\n",
      "2      5    3       Ozone     12.0\n",
      "3      5    4       Ozone     18.0\n",
      "4      5    5       Ozone      NaN\n",
      "measurement  Ozone  Solar.R  Temp  Wind\n",
      "Month Day                              \n",
      "5     1       41.0    190.0  67.0   7.4\n",
      "      2       36.0    118.0  72.0   8.0\n",
      "      3       12.0    149.0  74.0  12.6\n",
      "      4       18.0    313.0  62.0  11.5\n",
      "      5        NaN      NaN  56.0  14.3\n"
     ]
    }
   ],
   "source": [
    "# Print the head of airquality_melt\n",
    "print(airquality_melt.head())\n",
    "\n",
    "# Pivot airquality_melt: airquality_pivot\n",
    "airquality_pivot = airquality_melt.pivot_table(index=['Month', 'Day'], columns='measurement', values='reading')\n",
    "\n",
    "# Print the head of airquality_pivot\n",
    "print(airquality_pivot.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "### Resetting the index of a DataFrame\n",
    "After pivoting `` airquality_melt `` in the previous exercise, you didn&amp;apos;t quite get back the original DataFrame.\n",
    "What you got back instead was a pandas DataFrame with a [hierarchical index (also known as a MultiIndex)](http://pandas.pydata.org/pandas-docs/stable/advanced.html).\n",
    "Hierarchical indexes are covered in depth in [Manipulating DataFrames with pandas](https://www.datacamp.com/courses/manipulating-dataframes-with-pandas).In essence, they allow you to group columns or rows by another variable - in this case, by `` &amp;apos;Month&amp;apos; `` as well as `` &amp;apos;Day&amp;apos; ``. \n",
    "There&amp;apos;s a very simple method you can use to get back the original DataFrame from the pivoted DataFrame: `` .reset_index() ``. Dan didn&amp;apos;t show you how to use this method in the video, but you&amp;apos;re now going to practice using it in this exercise to get back the original DataFrame from `` airquality_pivot ``, which has been pre-loaded.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Print the index of `` airquality_pivot `` by accessing its `` .index `` attribute. This has been done for you.\n",
    "*   Reset the index of `` airquality_pivot `` using its `` .reset_index() `` method.\n",
    "*   Print the new index of `` airquality_pivot ``.\n",
    "*   Print the head of `` airquality_pivot ``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measurement  Month  Day  Ozone  Solar.R  Temp  Wind\n",
      "0                5    1   41.0    190.0  67.0   7.4\n",
      "1                5    2   36.0    118.0  72.0   8.0\n",
      "2                5    3   12.0    149.0  74.0  12.6\n",
      "3                5    4   18.0    313.0  62.0  11.5\n",
      "4                5    5    NaN      NaN  56.0  14.3\n",
      "   Ozone  Solar.R  Wind  Temp  Month  Day\n",
      "0   41.0    190.0   7.4    67      5    1\n",
      "1   36.0    118.0   8.0    72      5    2\n",
      "2   12.0    149.0  12.6    74      5    3\n",
      "3   18.0    313.0  11.5    62      5    4\n",
      "4    NaN      NaN  14.3    56      5    5\n"
     ]
    }
   ],
   "source": [
    "# Pivot airquality_dup: airquality_pivot\n",
    "#airquality_pivot = airquality_dup.pivot_table(index=['Month', 'Day'], columns='measurement', values='reading', aggfunc=np.mean)\n",
    "\n",
    "# Reset the index of airquality_pivot\n",
    "airquality_pivot = airquality_pivot.reset_index()\n",
    "\n",
    "# Print the head of airquality_pivot\n",
    "print(airquality_pivot.head())\n",
    "\n",
    "# Print the head of airquality\n",
    "print(airquality.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise 8\n",
    "### Pivoting duplicate values\n",
    "So far, you&amp;apos;ve used the `` .pivot_table() `` method when there are multiple `` index `` values you want to hold constant during a pivot. In the video, Dan showed you how you can also use pivot tables to deal with duplicate values by providing an aggregation function through the `` aggfunc `` parameter. Here, you&amp;apos;re going to combine both these uses of pivot tables.\n",
    "Let&amp;apos;s say your data collection method accidentally duplicated your dataset. Such a dataset, in which each row is duplicated, has been pre-loaded as `` airquality_dup ``. In addition, the `` airquality_melt `` DataFrame from the previous exercise has been pre-loaded. Explore their shapes in the IPython Shell by accessing their `` .shape `` attributes to confirm the duplicate rows present in `` airquality_dup ``.\n",
    "You&amp;apos;ll see that by using `` .pivot_table() `` and the `` aggfunc `` parameter, you can not only reshape your data, but also remove duplicates. Finally, you can then flatten the columns of the pivoted DataFrame using `` .reset_index() ``.\n",
    "NumPy and pandas have been imported as `` np `` and `` pd `` respectively.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Pivot `` airquality_dup `` by using `` .pivot_table() `` with the rows indexed by `` &amp;apos;Month&amp;apos; `` and `` &amp;apos;Day&amp;apos; ``, the columns indexed by `` &amp;apos;measurement&amp;apos; ``, and the values populated with `` &amp;apos;reading&amp;apos; ``. Use `` np.mean `` for the aggregation function. \n",
    "*   Print the head of `` airquality_pivot ``.\n",
    "*   Flatten `` airquality_pivot `` by resetting its index.\n",
    "*   Print the head of `` airquality_pivot `` and then the original `` airquality `` DataFrame to compare their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=153, step=1)\n",
      "RangeIndex(start=0, stop=153, step=1)\n",
      "measurement  index  Month  Day  Ozone  Solar.R  Temp  Wind\n",
      "0                0      5    1   41.0    190.0  67.0   7.4\n",
      "1                1      5    2   36.0    118.0  72.0   8.0\n",
      "2                2      5    3   12.0    149.0  74.0  12.6\n",
      "3                3      5    4   18.0    313.0  62.0  11.5\n",
      "4                4      5    5    NaN      NaN  56.0  14.3\n"
     ]
    }
   ],
   "source": [
    "# Print the index of airquality_pivot\n",
    "print(airquality_pivot.index)\n",
    "\n",
    "# Reset the index of airquality_pivot: airquality_pivot_reset\n",
    "airquality_pivot_reset = airquality_pivot.reset_index()\n",
    "\n",
    "# Print the head of airquality_pivot\n",
    "print(airquality_pivot_reset.index)\n",
    "\n",
    "# Print the head of airquality\n",
    "print(airquality_pivot_reset.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 9\n",
    "\n",
    "### Splitting a column with .str\n",
    "\n",
    "The dataset you saw in the video, consisting of case counts of tuberculosis by country, year, gender, and age group, has been pre-loaded into a DataFrame as `` tb ``.\n",
    "In this exercise, you&amp;apos;re going to tidy the `` m014 `` column, which represents males aged 0-14 years of age.In order to parse this value, you need to extract the first letter into a new column for `` gender ``, and the rest into a column for `` age_group ``.Here, since you can parse values by position, you can take advantage of pandas&amp;apos; vectorized string slicing by using the `` str `` attribute of columns of type `` object ``.\n",
    "Begin by printing the columns of `` tb `` in the IPython Shell using its `` .columns `` attribute, and take note of the problematic column.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Melt `` tb `` keeping `` country `` and ``year `` fixed.\n",
    "*   Create a ``gender `` column by slicing the first letter of the `` variable `` column of `` tb_melt ``.\n",
    "*   Create an `` age_group `` column by slicing the rest of the `` variable `` column of `` tb_melt ``.\n",
    "*   Print the head of `` tb_melt ``. This has been done for you, so hit Submit Answerto see the results!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country  year variable  value gender age_group\n",
      "0      AD  2000     m014    0.0      m       014\n",
      "1      AE  2000     m014    2.0      m       014\n",
      "2      AF  2000     m014   52.0      m       014\n",
      "3      AG  2000     m014    0.0      m       014\n",
      "4      AL  2000     m014    2.0      m       014\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "tb=pd.read_csv('tb.csv')\n",
    "\n",
    "# Melt tb: tb_melt\n",
    "tb_melt = pd.melt(frame=tb, id_vars=['country', 'year'])\n",
    "\n",
    "# Create the 'gender' column\n",
    "tb_melt['gender'] = tb_melt.variable.str[0]\n",
    "\n",
    "# Create the 'age_group' column\n",
    "tb_melt['age_group'] = tb_melt.variable.str[1:]\n",
    "\n",
    "# Print the head of tb_melt\n",
    "print(tb_melt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise 10\n",
    "### Splitting a column with .split() and .get()\n",
    "Another common way __multiple variables are stored in columns__ is with a delimiter. You&amp;apos;ll learn how to deal with such cases in this exercise,using a [dataset consisting of Ebola cases and death counts by state and country](https://data.humdata.org/dataset/ebola-cases-2014). It has been pre-loaded into a DataFrame as `` ebola ``.\n",
    "Print the columns of `` ebola `` in the IPython Shell using `` ebola.columns ``. Notice that the data has column names such as `` Cases_Guinea `` and `` Deaths_Guinea ``. Here,the underscore `` _ `` serves as a delimiter between the first part (cases or deaths), and the second part (country).\n",
    "This time, you cannot directly slice the variable by position as in the previous exercise. You now need to use Python&amp;apos;s built-in string method called `` .split() ``. By default,this method will split a string into parts separated by a space. However, in this case you want it to split by an underscore. You can do this on `` Cases_Guinea ``, for example,using `` Cases_Guinea.split(&amp;apos;_&amp;apos;) ``, which returns the list `` [&amp;apos;Cases&amp;apos;, &amp;apos;Guinea&amp;apos;] ``.\n",
    "The next challenge is to extract the first element of this list and assign it to a `` type `` variable, and the second element of the list to a `` country `` variable. You can accomplish this by accessing the `` str `` attribute of the column and using the `` .get() `` method to retrieve the `` 0 `` or `` 1 `` index, depending on the part you want.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Melt `` ebola `` using `` &amp;apos;Date&amp;apos; `` and `` &amp;apos;Day&amp;apos; `` as the `` id_vars ``, `` &amp;apos;type_country&amp;apos; `` as the `` var_name ``, and `` &amp;apos;counts&amp;apos; `` as the `` value_name ``.\n",
    "*   Create a column called `` &amp;apos;str_split&amp;apos; `` by splitting the `` &amp;apos;type_country&amp;apos; `` column of `` ebola_melt `` on `` &amp;apos;_&amp;apos; ``. Note that you will first have to access the `` str `` attribute of `` type_country `` before you can use `` .split() ``.\n",
    "*   Create a column called `` &amp;apos;type&amp;apos; `` by using the `` .get() `` method to retrieve index `` 0 `` of the `` &amp;apos;str_split&amp;apos; `` column of `` ebola_melt ``.\n",
    "*   Create a column called `` &amp;apos;country&amp;apos; `` by using the `` .get() `` method to retrieve index `` 1 `` of the `` &amp;apos;str_split&amp;apos; `` column of `` ebola_melt ``.\n",
    "*   Print the head of `` ebola ``. This has been done for you, so hit &amp;apos;Submit Answer&amp;apos; to view the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day  type_country  counts        str_split   type country\n",
      "0    1/5/2015  289  Cases_Guinea    2776  [Cases, Guinea]  Cases  Guinea\n",
      "1    1/4/2015  288  Cases_Guinea    2775  [Cases, Guinea]  Cases  Guinea\n",
      "2    1/3/2015  287  Cases_Guinea    2769  [Cases, Guinea]  Cases  Guinea\n",
      "3    1/2/2015  286  Cases_Guinea       0  [Cases, Guinea]  Cases  Guinea\n",
      "4  12/31/2014  284  Cases_Guinea    2730  [Cases, Guinea]  Cases  Guinea\n"
     ]
    }
   ],
   "source": [
    "ebola=pd.read_csv('ebola.csv')\n",
    "\n",
    "\n",
    "# Melt ebola: ebola_melt\n",
    "ebola_melt = pd.melt(ebola, id_vars=['Date', 'Day'], var_name='type_country', value_name='counts')\n",
    "\n",
    "# Create the 'str_split' column\n",
    "ebola_melt['str_split'] = ebola_melt.type_country.str.split('_')\n",
    "\n",
    "# Create the 'type' column\n",
    "ebola_melt['type'] = ebola_melt.str_split.str.get(0)\n",
    "\n",
    "# Create the 'country' column\n",
    "ebola_melt['country'] = ebola_melt.str_split.str.get(1)\n",
    "\n",
    "# Print the head of ebola_melt\n",
    "print(ebola_melt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining data for analysis\n",
    "### Exercise 1\n",
    "### Combining rows of data\n",
    "The dataset you&amp;apos;ll be working with here relates to [NYC Uber data](http://data.beta.nyc/dataset/uber-trip-data-foiled-apr-sep-2014).The original dataset has all the originating Uber pickup locations by time and latitude and longitude.For didactic purposes, you&amp;apos;ll be working with a very small portion of the actual data.\n",
    "Three DataFrames have been pre-loaded: `` uber1 ``, which contains data for April 2014, `` uber2 ``, which contains data for May 2014, and `` uber3 ``, which contains data for June 2014. Your job in this exerciseis to concatenate these DataFrames together such that the resulting DataFrame has the data for all three months.\n",
    "Begin by exploring the structure of these three DataFrames in the IPython Shell using methods such as `` .head() ``.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Concatenate `` uber1 ``, `` uber2 ``, and `` uber3 `` together using `` pd.concat() ``. You&amp;apos;ll have to pass the DataFrames in as a list.\n",
    "*   Print the shape and then the head of the concatenated DataFrame, `` row_concat ``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(295, 5)\n",
      "   Unnamed: 0         Date/Time      Lat      Lon    Base\n",
      "0           0  4/1/2014 0:11:00  40.7690 -73.9549  B02512\n",
      "1           1  4/1/2014 0:17:00  40.7267 -74.0345  B02512\n",
      "2           2  4/1/2014 0:21:00  40.7316 -73.9873  B02512\n",
      "3           3  4/1/2014 0:28:00  40.7588 -73.9776  B02512\n",
      "4           4  4/1/2014 0:33:00  40.7594 -73.9722  B02512\n"
     ]
    }
   ],
   "source": [
    "uber=pd.read_csv('nyc_uber_2014.csv')\n",
    "uber1=uber[0:99]\n",
    "uber2=uber[100:199]\n",
    "uber3=uber[200:297]\n",
    "# Concatenate uber1, uber2, and uber3: row_concat\n",
    "row_concat = pd.concat([uber1,uber2,uber3])\n",
    "\n",
    "# Print the shape of row_concat\n",
    "print(row_concat.shape)\n",
    "\n",
    "# Print the head of row_concat\n",
    "print(row_concat.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "### Combining columns of data\n",
    "Think of column-wise concatenation of data as stitching data together from the sides instead of the top and bottom.To perform this action, you use the same `` pd.concat() `` function, but this time with the keyword argument `` axis=1 ``. The default, `` axis=0 ``, is for a row-wise concatenation.\n",
    "You&amp;apos;ll return to the [Ebola dataset](https://data.humdata.org/dataset/ebola-cases-2014) you worked with briefly in the last chapter. It has been pre-loaded intoa DataFrame called `` ebola_melt ``. In this DataFrame, the status and country of a patient is contained in a single column. This column has been parsed into a new DataFrame, `` status_country ``, where there are separate columns for status and country. \n",
    "Explore the `` ebola_melt `` and `` status_country `` DataFrames in the IPython Shell. Your job is to concatenate them column-wise in order to obtain a final, clean DataFrame.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Concatenate `` ebola_melt `` and `` status_country `` column-wise into a single DataFrame called `` ebola_tidy ``. Be sure to specify `` axis=1 `` and to pass the two DataFrames in as a list.\n",
    "*   Print the shape and then the head of the concatenated DataFrame, `` ebola_tidy ``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1952 entries, 0 to 1951\n",
      "Data columns (total 4 columns):\n",
      "Date              1952 non-null object\n",
      "Day               1952 non-null int64\n",
      "status_country    1952 non-null object\n",
      "counts            1952 non-null int64\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 61.1+ KB\n"
     ]
    }
   ],
   "source": [
    "ebola = pd.read_csv(\"Ebola.csv\")\n",
    "# Melt ebola: ebola_melt\n",
    "ebola_melt = pd.melt(ebola, id_vars=['Date', 'Day'], var_name='status_country', value_name='counts')\n",
    "ebola_melt.info()\n",
    "ebola_melt.to_csv(\"ebola_melt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1952, 6)\n",
      "         Date  Day status_country  counts status country\n",
      "0  01-05-2015  289   Cases_Guinea    2776  Cases  Guinea\n",
      "1  01-04-2015  288   Cases_Guinea    2775  Cases  Guinea\n",
      "2  01-03-2015  287   Cases_Guinea    2769  Cases  Guinea\n",
      "3  01-02-2015  286   Cases_Guinea       0  Cases  Guinea\n",
      "4  12/31/2014  284   Cases_Guinea    2730  Cases  Guinea\n"
     ]
    }
   ],
   "source": [
    "# Author Entry\n",
    "ebola = pd.read_csv(\"Ebola.csv\")\n",
    "# Melt ebola: ebola_melt\n",
    "ebola_melt = pd.melt(ebola, id_vars=['Date', 'Day'], var_name='status_country', value_name='counts')\n",
    "status_country = ebola_melt.status_country.str.split(\"_\",expand=True)\n",
    "status_country.columns = [\"status\",\"country\"]\n",
    "######################\n",
    "\n",
    "\n",
    "# Concatenate ebola_melt and status_country column-wise: ebola_tidy\n",
    "ebola_tidy = pd.concat([ebola_melt,status_country ], axis= 1)\n",
    "\n",
    "# Print the shape of ebola_tidy\n",
    "print(ebola_tidy.shape)\n",
    "\n",
    "# Print the head of ebola_tidy\n",
    "print(ebola_tidy.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter  -g Exercise 3\n",
    "### Combining columns of data\n",
    "\n",
    "Think of column-wise concatenation of data as stitching data together from the sides instead of the top and bottom.\n",
    "To perform this action, you use the same `` pd.concat() `` function, but this time with the keyword argument `` axis=1 ``.\n",
    "The default, `` axis= 0 ``, is for a row-wise concatenation.\n",
    "You&amp;apos;ll return to the [Ebola dataset](https://data.humdata.org/dataset/ebola-cases-2014) you worked with briefly in the last chapter. It has been pre-loaded intoa DataFrame called `` ebola_melt ``. In this DataFrame, the status and country of a patient is contained in a single column. This column has been parsed into a new DataFrame, `` status_country ``, where there are separate columns for status and country. \n",
    "Explore the `` ebola_melt `` and `` status_country `` DataFrames in the IPython Shell. Your job is to concatenate them column-wise in order to obtain a final, clean DataFrame.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Concatenate `` ebola_melt `` and `` status_country `` column-wise into a single DataFrame called `` ebola_tidy ``. Be sure to specify `` axis=1 `` and to pass the two DataFrames in as a list.\n",
    "*   Print the shape and then the head of the concatenated DataFrame, `` ebola_tidy ``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date  Day status_country  counts\n",
      "0     01-05-2015  289   Cases_Guinea    2776\n",
      "1     01-04-2015  288   Cases_Guinea    2775\n",
      "2     01-03-2015  287   Cases_Guinea    2769\n",
      "3     01-02-2015  286   Cases_Guinea       0\n",
      "4     12/31/2014  284   Cases_Guinea    2730\n",
      "5     12/28/2014  281   Cases_Guinea    2706\n",
      "6     12/27/2014  280   Cases_Guinea    2695\n",
      "7     12/24/2014  277   Cases_Guinea    2630\n",
      "8     12/21/2014  273   Cases_Guinea    2597\n",
      "9     12/20/2014  272   Cases_Guinea    2571\n",
      "10    12/18/2014  271   Cases_Guinea       0\n",
      "11    12/14/2014  267   Cases_Guinea    2416\n",
      "12    12-09-2014  262   Cases_Guinea       0\n",
      "13    12-07-2014  260   Cases_Guinea    2292\n",
      "14    12-03-2014  256   Cases_Guinea       0\n",
      "15    11/30/2014  253   Cases_Guinea    2164\n",
      "16    11/28/2014  251   Cases_Guinea       0\n",
      "17    11/23/2014  246   Cases_Guinea    2134\n",
      "18    11/22/2014  245   Cases_Guinea       0\n",
      "19    11/18/2014  241   Cases_Guinea    2047\n",
      "20    11/16/2014  239   Cases_Guinea    1971\n",
      "21    11/15/2014  238   Cases_Guinea       0\n",
      "22    11-11-2014  234   Cases_Guinea    1919\n",
      "23    11-10-2014  233   Cases_Guinea       0\n",
      "24    11-09-2014  232   Cases_Guinea    1878\n",
      "25    11-08-2014  231   Cases_Guinea       0\n",
      "26    11-04-2014  227   Cases_Guinea       0\n",
      "27    11-03-2014  226   Cases_Guinea    1760\n",
      "28    11-02-2014  225   Cases_Guinea    1731\n",
      "29    10/31/2014  222   Cases_Guinea       0\n",
      "...          ...  ...            ...     ...\n",
      "1922   5/23/2014   62    Deaths_Mali       0\n",
      "1923  05-12-2014   51    Deaths_Mali       0\n",
      "1924  05-10-2014   49    Deaths_Mali       0\n",
      "1925  05-07-2014   46    Deaths_Mali       0\n",
      "1926  05-05-2014   44    Deaths_Mali       0\n",
      "1927  05-03-2014   42    Deaths_Mali       0\n",
      "1928  05-01-2014   40    Deaths_Mali       0\n",
      "1929   4/26/2014   35    Deaths_Mali       0\n",
      "1930   4/24/2014   33    Deaths_Mali       0\n",
      "1931   4/23/2014   32    Deaths_Mali       0\n",
      "1932   4/22/2014   31    Deaths_Mali       0\n",
      "1933   4/21/2014   30    Deaths_Mali       0\n",
      "1934   4/20/2014   29    Deaths_Mali       0\n",
      "1935   4/17/2014   26    Deaths_Mali       0\n",
      "1936   4/16/2014   25    Deaths_Mali       0\n",
      "1937   4/15/2014   24    Deaths_Mali       0\n",
      "1938   4/14/2014   23    Deaths_Mali       0\n",
      "1939  04-11-2014   20    Deaths_Mali       0\n",
      "1940  04-09-2014   18    Deaths_Mali       0\n",
      "1941  04-07-2014   16    Deaths_Mali       0\n",
      "1942  04-04-2014   13    Deaths_Mali       0\n",
      "1943  04-01-2014   10    Deaths_Mali       0\n",
      "1944   3/31/2014    9    Deaths_Mali       0\n",
      "1945   3/29/2014    7    Deaths_Mali       0\n",
      "1946   3/28/2014    6    Deaths_Mali       0\n",
      "1947   3/27/2014    5    Deaths_Mali       0\n",
      "1948   3/26/2014    4    Deaths_Mali       0\n",
      "1949   3/25/2014    3    Deaths_Mali       0\n",
      "1950   3/24/2014    2    Deaths_Mali       0\n",
      "1951   3/22/2014    0    Deaths_Mali       0\n",
      "\n",
      "[1952 rows x 4 columns]\n",
      "(1952, 6)\n",
      "         Date  Day status_country  counts status country\n",
      "0  01-05-2015  289   Cases_Guinea    2776  Cases  Guinea\n",
      "1  01-04-2015  288   Cases_Guinea    2775  Cases  Guinea\n",
      "2  01-03-2015  287   Cases_Guinea    2769  Cases  Guinea\n",
      "3  01-02-2015  286   Cases_Guinea       0  Cases  Guinea\n",
      "4  12/31/2014  284   Cases_Guinea    2730  Cases  Guinea\n"
     ]
    }
   ],
   "source": [
    "# Concatenate ebola_melt and status_country column-wise: ebola_tidy\n",
    "ebola_tidy = pd.concat([ebola_melt, status_country], axis=1)\n",
    "print(ebola_melt)\n",
    "# Print the shape of ebola_tidy\n",
    "print(ebola_tidy.shape)\n",
    "\n",
    "# Print the head of ebola_tidy\n",
    "print(ebola_tidy.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter  -g Exercise 5\n",
    "### Finding files that match a pattern\n",
    "You&amp;apos;re now going to practice using the `` glob `` module to find all csv files in the workspace. In the next exercise, you&amp;apos;ll programmatically load them into DataFrames.\n",
    "As Dan showed you in the video, the `` glob `` module has a function called `` glob `` that takes a pattern and returns a list of the files in the working directory that match that pattern.\n",
    "For example, if you know the pattern is `` part_ `` `` single digit number `` `` .csv ``, you can write the pattern as `` &amp;apos;part_?.csv&amp;apos; `` (which would match `` part_1.csv ``, `` part_2.csv ``, `` part_3.csv ``, etc.)\n",
    "Similarly, you can find all `` .csv `` files with `` &amp;apos;*.csv&amp;apos; ``, or all parts with `` &amp;apos;part_*&amp;apos; ``. The `` ? `` wildcard represents any 1 character, and the `` * `` wildcard represents any number of characters.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Import the `` glob `` module along with `` pandas `` (as its usual alias `` pd ``).\n",
    "*   Write a pattern to match all `` .csv `` files.\n",
    "*   Save all files that match the pattern using the `` glob() `` function within the `` glob `` module. That is, by using `` glob.glob() ``.\n",
    "*   Print the list of file names. This has been done for you.\n",
    "*   Read the second file in `` csv_files `` (i.e., index `` 1 ``) into a DataFrame called `` csv2 ``.\n",
    "*   Hit &amp;apos;Submit Answer&amp;apos; to print the head of `` csv2 ``. Does it look familiar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uber-raw-data-2014_04.csv', 'uber-raw-data-2014_05.csv', 'uber-raw-data-2014_06.csv']\n",
      "          Date/Time      Lat      Lon    Base\n",
      "0  5/1/2014 0:02:00  40.7521 -73.9914  B02512\n",
      "1  5/1/2014 0:06:00  40.6965 -73.9715  B02512\n",
      "2  5/1/2014 0:15:00  40.7464 -73.9838  B02512\n",
      "3  5/1/2014 0:17:00  40.7463 -74.0011  B02512\n",
      "4  5/1/2014 0:17:00  40.7594 -73.9734  B02512\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Write the pattern: pattern\n",
    "pattern = 'uber*.csv'\n",
    "\n",
    "# Save all file matches: csv_files\n",
    "csv_files = glob.glob(pattern)\n",
    "\n",
    "# Print the file names\n",
    "print(csv_files)\n",
    "\n",
    "# Load the second file into a DataFrame: csv2\n",
    "csv2 = pd.read_csv(csv_files[1])\n",
    "\n",
    "# Print the head of csv2\n",
    "print(csv2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter  -g Exercise 6\n",
    "### Iterating and concatenating all matches\n",
    "Now that you have a list of filenames to load,you can load all the files into a list of DataFrames that can then be concatenated.\n",
    "You&amp;apos;ll start with an empty list called `` frames ``. Your job is to use a `` for `` loopto:\n",
    "You can then concatenate this list of DataFrames using `` pd.concat() ``. Go for it!\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Write a `` for `` loop to iterate through `` csv_files ``:\n",
    "    \n",
    "    *   In each iteration of the loop, read `` csv `` into a DataFrame called `` df ``.\n",
    "    *   After creating `` df ``, append it to the list `` frames `` using the `` .append() `` method.\n",
    "    \n",
    "    \n",
    "    \n",
    "*   Concatenate `` frames `` into a single DataFrame called `` uber ``.\n",
    "*   Hit &amp;apos;Submit Answer&amp;apos; to see the head and shape of the concatenated DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uber-raw-data-2014_04.csv\n",
      "uber-raw-data-2014_05.csv\n",
      "uber-raw-data-2014_06.csv\n",
      "(300, 4)\n",
      "          Date/Time      Lat      Lon    Base\n",
      "0  4/1/2014 0:11:00  40.7690 -73.9549  B02512\n",
      "1  4/1/2014 0:17:00  40.7267 -74.0345  B02512\n",
      "2  4/1/2014 0:21:00  40.7316 -73.9873  B02512\n",
      "3  4/1/2014 0:28:00  40.7588 -73.9776  B02512\n",
      "4  4/1/2014 0:33:00  40.7594 -73.9722  B02512\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create an empty list: frames\n",
    "frames = []\n",
    "\n",
    "\n",
    "#  Iterate over csv_files\n",
    "for csv in csv_files:\n",
    "    print(csv)\n",
    "    if csv == \"gg.csv\":\n",
    "        continue\n",
    "    #  Read csv into a DataFrame: df\n",
    "    df = pd.read_csv(csv)\n",
    "    \n",
    "    # Append df to frames\n",
    "    frames.append(df)\n",
    "\n",
    "# Concatenate frames into a single DataFrame: uber\n",
    "uber = pd.concat(frames)\n",
    "\n",
    "# Print the shape of uber\n",
    "print(uber.shape)\n",
    "\n",
    "# Print the head of uber\n",
    "print(uber.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter  -g Exercise 8\n",
    "### 1-to-1 data merge\n",
    "Merging data allows you to combine disparate datasets into a single dataset to do more complex analysis.\n",
    "Here, you&amp;apos;ll be using survey data that contains readings that William Dyer, Frank Pabodie, and Valentina Roerich took in the late 1920 and 1930 while they were on an expedition towards Antarctica.The dataset was taken from a sqlite database from the [Software Carpentry SQL lesson](http://swcarpentry.github.io/sql-novice-survey/). \n",
    "Two DataFrames have been pre-loaded: `` site `` and `` visited ``. Explore them in the IPython Shell and take note of their structure and column names. Your task is to perform a 1-to-1 merge of these two DataFrames usingthe `` name`` column of `` site `` and the `` site `` column of `` visited ``.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Merge the `` site `` and `` visited `` DataFrames on the `` name `` column of `` site `` and `` \\site `` column of `` visited ``.\n",
    "*   Print the merged DataFrame `` o2o ``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    name    lat    long  ident   site       dated\n",
      "0   DR-1 -49.85 -128.57    619   DR-1  08-02-1927\n",
      "1   DR-3 -47.15 -126.72    734   DR-3  07-01-1939\n",
      "2  MSK-4 -48.87 -123.40    837  MSK-4  14-01-1932\n"
     ]
    }
   ],
   "source": [
    "site = pd.read_csv('site.csv')\n",
    "visited = pd.read_csv('visited.csv')\n",
    "# Merge the DataFrames: o2o\n",
    "o2o = pd.merge(left=site, right=visited, left_on='name', right_on='site')\n",
    "\n",
    "# Print m2o\n",
    "print(o2o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter  -g Exercise 9\n",
    "### Many-to-1 data merge\n",
    "In a many-to-one (or one-to-many) merge, one of the values will be duplicated and recycled in the output.That is, one of the keys in the merge is not unique. \n",
    "Here, the two DataFrames `` site `` and `` visited `` have been pre-loaded once again.Note that this time, `` visited `` has multiple entries for the `` site `` column. Confirm this by exploring it in the IPython Shell.\n",
    "The `` .merge() `` method call is the same as the 1-to-1 merge from the previous exercise, but the data and output will be different.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Merge the `` site `` and `` visited `` DataFrames on the `` &amp;apos;name&amp;apos; `` column of `` site `` and `` &amp;apos;site&amp;apos; `` column of `` visited ``, exactly as you did in the previous exercise.\n",
    "*   Print the merged DataFrame and then hit &amp;apos;Submit Answer&amp;apos; to see the different output produced by this merge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    name    lat    long  ident   site       dated\n",
      "0   DR-1 -49.85 -128.57    619   DR-1  1927-02-08\n",
      "1   DR-1 -49.85 -128.57    622   DR-1  1927-02-10\n",
      "2   DR-1 -49.85 -128.57    844   DR-1  1932-03-22\n",
      "3   DR-3 -47.15 -126.72    734   DR-3  1939-01-07\n",
      "4   DR-3 -47.15 -126.72    735   DR-3  1930-01-12\n",
      "5   DR-3 -47.15 -126.72    751   DR-3  1930-02-26\n",
      "6   DR-3 -47.15 -126.72    752   DR-3         NaN\n",
      "7  MSK-4 -48.87 -123.40    837  MSK-4  1932-01-14\n"
     ]
    }
   ],
   "source": [
    "visited = pd.read_csv('visited2.csv')\n",
    "# Merge the DataFrames: m2o\n",
    "m2o = pd.merge(left=site, right=visited, left_on='name', right_on='site')\n",
    "\n",
    "# Print m2o\n",
    "print(m2o)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter  -g Exercise 10\n",
    "### Many-to-many data merge\n",
    "The final merging scenario occurs when both DataFrames do not have unique keys for a merge.What happens here is that for each duplicated key, every pairwise combination will be created.\n",
    "Two example DataFrames that share common key values have been pre-loaded: `` df1 `` and `` df2 ``. Another DataFrame `` df3 ``,which is the result of `` df1 `` merged with `` df2 ``, has been pre-loaded. All three DataFrames have been printed - look at the output and notice how pairwise combinations have been created. This example is to help youdevelop your intuition for many-to-many merges.\n",
    "Here, you&amp;apos;ll work with the `` site `` and `` visited `` DataFrames from before, and a new `` survey `` DataFrame. Your taskis to merge `` site `` and `` visited `` as you did in the earlier exercises. You will then merge this merged DataFrame with `` survey ``.\n",
    "Begin by exploring the `` site ``, `` visited ``, and `` survey `` DataFrames in the IPython Shell.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Merge the `` site `` and `` visited `` DataFrames on the `` &amp;apos;name&amp;apos; `` column of `` site `` and `` &amp;apos;site&amp;apos; `` column of `` visited ``, exactly as you did in the previous two exercises. Save the result as `` m2m ``.\n",
    "*   Merge the `` m2m `` and `` survey `` DataFrames on the `` &amp;apos;ident&amp;apos; `` column of `` m2m `` and `` &amp;apos;taken&amp;apos; `` column of `` survey ``.\n",
    "*   Hit &amp;apos;Submit Answer&amp;apos; to print the first 20 lines of the merged DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     name    lat    long  ident   site       dated  taken person quant  \\\n",
      "0    DR-1 -49.85 -128.57    619   DR-1  1927-02-08    619   dyer   rad   \n",
      "1    DR-1 -49.85 -128.57    619   DR-1  1927-02-08    619   dyer   sal   \n",
      "2    DR-1 -49.85 -128.57    622   DR-1  1927-02-10    622   dyer   rad   \n",
      "3    DR-1 -49.85 -128.57    622   DR-1  1927-02-10    622   dyer   sal   \n",
      "4    DR-1 -49.85 -128.57    844   DR-1  1932-03-22    844    roe   rad   \n",
      "5    DR-3 -47.15 -126.72    734   DR-3  1939-01-07    734     pb   rad   \n",
      "6    DR-3 -47.15 -126.72    734   DR-3  1939-01-07    734   lake   sal   \n",
      "7    DR-3 -47.15 -126.72    734   DR-3  1939-01-07    734     pb  temp   \n",
      "8    DR-3 -47.15 -126.72    735   DR-3  1930-01-12    735     pb   rad   \n",
      "9    DR-3 -47.15 -126.72    735   DR-3  1930-01-12    735    NaN   sal   \n",
      "10   DR-3 -47.15 -126.72    735   DR-3  1930-01-12    735    NaN  temp   \n",
      "11   DR-3 -47.15 -126.72    751   DR-3  1930-02-26    751     pb   rad   \n",
      "12   DR-3 -47.15 -126.72    751   DR-3  1930-02-26    751     pb  temp   \n",
      "13   DR-3 -47.15 -126.72    751   DR-3  1930-02-26    751   lake   sal   \n",
      "14   DR-3 -47.15 -126.72    752   DR-3         NaN    752   lake   rad   \n",
      "15   DR-3 -47.15 -126.72    752   DR-3         NaN    752   lake   sal   \n",
      "16   DR-3 -47.15 -126.72    752   DR-3         NaN    752   lake  temp   \n",
      "17   DR-3 -47.15 -126.72    752   DR-3         NaN    752    roe   sal   \n",
      "18  MSK-4 -48.87 -123.40    837  MSK-4  1932-01-14    837   lake   rad   \n",
      "19  MSK-4 -48.87 -123.40    837  MSK-4  1932-01-14    837   lake   sal   \n",
      "\n",
      "    reading  \n",
      "0      9.82  \n",
      "1      0.13  \n",
      "2      7.80  \n",
      "3      0.09  \n",
      "4     11.25  \n",
      "5      8.41  \n",
      "6      0.05  \n",
      "7    -21.50  \n",
      "8      7.22  \n",
      "9      0.06  \n",
      "10   -26.00  \n",
      "11     4.35  \n",
      "12   -18.50  \n",
      "13     0.10  \n",
      "14     2.19  \n",
      "15     0.09  \n",
      "16   -16.00  \n",
      "17    41.60  \n",
      "18     1.46  \n",
      "19     0.21  \n"
     ]
    }
   ],
   "source": [
    "visited = pd.read_csv('visited2.csv')\n",
    "survey = pd.read_csv('survey.csv')\n",
    "\n",
    "# Merge site and visited: m2m\n",
    "m2m = pd.merge(left=site, right=visited, left_on='name', right_on='site')\n",
    "\n",
    "# Merge m2m and survey: m2m\n",
    "m2m = pd.merge(left=m2m, right=survey, left_on='ident', right_on='taken')\n",
    "\n",
    "# Print the first 20 lines of m2m\n",
    "print(m2m.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter  -- Exercise 2\n",
    "### Converting data types\n",
    "In this exercise, you&amp;apos;ll see how ensuring all categorical variables in a DataFrame are of type `` category `` reduces memory usage.\n",
    "The [tips dataset](https://github.com/mwaskom/seaborn-data/blob/master/tips.csv) has been loaded into a DataFrame called `` tips ``. This data contains information about how much a customer tipped, whether the customer was male or female, a smoker or not, etc. \n",
    "Look at the output of `` tips.info() `` in the IPython Shell. You&amp;apos;ll note that two columns that should be categorical - `` sex `` and `` smoker `` - are instead of type `` object ``, which is pandas&amp;apos; way of storing arbitrary strings. Your job is to convert these two columns to type `` category `` and note the reduced memory usage.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Convert the `` sex `` column of the `` tips `` DataFrame to type `` &amp;apos;category&amp;apos; `` using the `` .astype() `` method.\n",
    "*   Convert the `` smoker `` column of the `` tips `` DataFrame.\n",
    "*   Print the memory usage of `` tips `` after converting the data types of the columns. Use the `` .info() `` method to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 244 entries, 0 to 243\n",
      "Data columns (total 7 columns):\n",
      "total_dollar    244 non-null float64\n",
      "tip             244 non-null float64\n",
      "sex             244 non-null category\n",
      "smoker          244 non-null category\n",
      "day             244 non-null object\n",
      "time            244 non-null object\n",
      "size            244 non-null int64\n",
      "dtypes: category(2), float64(2), int64(1), object(2)\n",
      "memory usage: 10.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tips=pd.read_csv('tips.csv')\n",
    "\n",
    "# Convert the sex column to type 'category'\n",
    "tips.sex = tips.sex.astype('category')\n",
    "\n",
    "# Convert the smoker column to type 'category'\n",
    "tips.smoker = tips.smoker.astype('category')\n",
    "\n",
    "# Print the info of tips\n",
    "print(tips.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter  -- Exercise 3\n",
    "### Working with numeric data\n",
    "If you expect the data type of a column to be numeric (`` int `` or `` float ``), but instead it is of type `` object ``,this typically means that there is a non numeric value in the column, which also signifies bad data.\n",
    "You can use the `` pd.to_numeric() `` function to convert a column into a numeric data type. If the functionraises an error, you can be sure that there is a bad value within the column. You can either use the techniquesyou learned in Chapter 1 to do some exploratory data analysis and find the bad value, or you can choose to ignore or `` coerce `` the value into a missing value, `` NaN ``.\n",
    "A modified version of the tips dataset has been pre-loaded into a DataFrame called `` tips ``. For instructional purposes, it has been pre-processed to introduce some &amp;apos;bad&amp;apos; data for you to clean. Use the `` .info() `` method to explore this. You&amp;apos;ll note that the `` total_bill `` and `` tip `` columns, which should be numeric, are instead of type `` object ``. Your job is to fix this.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Use `` pd.to_numeric() `` to convert the `` &amp;apos;total_bill&amp;apos; `` column of `` tips `` to a numeric data type. Coerce the errors to `` NaN `` by specifying the keyword argument `` errors=&amp;apos;coerce&amp;apos; ``.\n",
    "*   Convert the `` &amp;apos;tip&amp;apos; `` column of `` &amp;apos;tips&amp;apos; `` to a numeric data type exactly as you did for the `` &amp;apos;total_bill&amp;apos; `` column.\n",
    "*   Print the `` info `` of `` tips `` to confirm that the data types of `` &amp;apos;total_bill&amp;apos; `` and `` &amp;apos;tips&amp;apos; `` are numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 244 entries, 0 to 243\n",
      "Data columns (total 7 columns):\n",
      "total_bill    202 non-null float64\n",
      "tip           220 non-null float64\n",
      "sex           234 non-null category\n",
      "smoker        229 non-null category\n",
      "day           243 non-null category\n",
      "time          227 non-null category\n",
      "size          231 non-null float64\n",
      "dtypes: category(4), float64(3)\n",
      "memory usage: 7.0 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tips = pd.read_csv(\"tips_nan.csv\", dtype={'sex': 'category',\n",
    "                                           'smoker': 'category',\n",
    "                                           'day': 'category',\n",
    "                                           'time': 'category'})\n",
    "\n",
    "\n",
    "# Convert 'total_bill' to a numeric dtype\n",
    "tips['total_bill'] = pd.to_numeric(tips['total_bill'], errors='coerce')\n",
    "\n",
    "# Convert 'tip' to a numeric dtype\n",
    "tips['tip'] = pd.to_numeric(tips['tip'], errors='coerce')\n",
    "\n",
    "# Print the info of tips\n",
    "print(tips.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter  -- Exercise 5\n",
    "### String parsing with regular expressions\n",
    "In the video, Dan introduced you to the basics of regular expressions, which are powerful ways of defining patterns to match strings. This exercise will get you started with writing them.\n",
    "When working with data, it is sometimes necessary to write a regular expression to look for properly entered values.Phone numbers in a dataset is a common field that needs to be checked for validity. Your job in this exercise is to define a regular expression to match US phone numbers that fit the pattern of `` xxx-xxx-xxxx ``.\n",
    "The [regular expression module](https://docs.python.org/3/library/re.html) in python is `` re ``.When performing pattern matching on data, since the pattern will be used for a match across multiple rows,it&amp;apos;s better to compile the pattern first using `` re.compile() ``, and then use the compiled pattern to match values.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Import `` re ``.\n",
    "*   Compile a pattern that matches a phone number of the format `` xxx-xxx-xxxx ``.\n",
    "    \n",
    "    *   Use `` \\d{x} `` to match `` x `` digits. Here you&amp;apos;ll need to use it three times: twice to match `` 3 `` digits, and once to match `` 4 `` digits.\n",
    "    *   Place the regular expression inside `` re.compile() ``.\n",
    "    \n",
    "    \n",
    "    \n",
    "*   Using the `` .match() `` method on `` prog ``, check whether the pattern matches the string `` &amp;apos;123-456-7890&amp;apos; ``.\n",
    "*   Using the same approach, now check whether the pattern matches the string `` &amp;apos;1123-456-7890&amp;apos; ``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Import the regular expression module\n",
    "import re\n",
    "\n",
    "# Compile the pattern: prog\n",
    "prog = re.compile('\\d{3}-\\d{3}-\\d{4}')\n",
    "\n",
    "# See if the pattern matches\n",
    "result = prog.match('123-456-7890')\n",
    "print(bool(result))\n",
    "\n",
    "# See if the pattern matches\n",
    "result =  prog.match('1123-456-7890')\n",
    "print(bool(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter  -- Exercise 6\n",
    "### Extracting numerical values from strings\n",
    "Extracting numbers from strings is a common task, particularly when working with unstructured data or log files.\n",
    "Say you have the following string: `` &amp;apos;the recipe calls for 6 strawberries and 2 bananas&amp;apos; ``.\n",
    "It would be useful to extract the `` 6 `` and the `` 2 `` from this string to be saved for later use when comparing strawberry to banana ratios.\n",
    "When using a regular expression to extract multiple numbers (or multiple pattern matches, to be exact), you can use the `` re.findall() `` function. Dan did not discuss this in the video, but it is straightforward to use: You pass in a pattern and a string to `` re.findall() ``, and it will return a list of the matches.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Import `` re ``.\n",
    "*   Write a pattern that will find all the numbers in the following string: `` &amp;apos;the recipe calls for 10 strawberries and 1 banana&amp;apos; ``. To do this:\n",
    "    \n",
    "    *   Use the `` re.findall() `` function and pass it two arguments: the pattern, followed by the string.\n",
    "    *   `` \\d `` is the pattern required to find digits. This should be followed with a `` + `` so that the previous element is matched one or more times. This ensures that `` 10 `` is viewed as one number and not as `` 1 `` and `` 0 ``.\n",
    "    \n",
    "    \n",
    "    \n",
    "*   Print the matches to confirm that your regular expression found the values `` 10 `` and `` 1 ``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the regular expression module\n",
    "import re\n",
    "\n",
    "# Find the numeric values: matches\n",
    "matches = re.findall('\\d+', 'the recipe calls for 10 strawberries and 1 banana')\n",
    "\n",
    "# Print the matches\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Chapter  -- Exercise 7\n",
    "### Pattern matching\n",
    "In this exercise, you&amp;apos;ll continue practicing your regular expression skills. For each provided string, your job is to write the appropriate pattern to match it.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Write patterns to match:\n",
    "    \n",
    "    *   A telephone number of the format `` xxx-xxx-xxxx ``. You already did this in a previous exercise.\n",
    "    *   A string of the format: A dollar sign, an arbitrary number of digits, a decimal point, 2 digits.\n",
    "        \n",
    "         *   Use `` \\$ `` to match the dollar sign, `` \\d* `` to match an arbitrary number of digits, `` \\. `` to match the                   decimal point, and `` \\d{x} `` to match `` x `` number of digits.\n",
    "    \n",
    "    * A capital letter, followed by an arbitrary number of alphanumeric characters.\n",
    "        \n",
    "    * Use `` [A-Z] `` to match any capital letter followed by `` \\w* `` to match an arbitrary number of alphanumeric characters.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the first pattern\n",
    "pattern1 = bool(re.match(pattern='\\d{3}-\\d{3}-\\d{4}', string='123-456-7890'))\n",
    "print(pattern1)\n",
    "\n",
    "# Write the second pattern\n",
    "pattern2 = bool(re.match(pattern='^\\$\\d*\\.\\d{2}$', string='$123.45'))\n",
    "print(pattern2)\n",
    "\n",
    "# Write the third pattern\n",
    "pattern3 = bool(re.match(pattern='w*', string='Australia'))\n",
    "print(pattern3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter  -- Exercise 9\n",
    "### Custom functions to clean data\n",
    "You'll now practice writing functions to clean data. \n",
    "The tips dataset has been pre-loaded into a DataFrame called `` tips ``. It has a `` &amp;apos;sex&amp;apos; `` column that contains the values `` &amp;apos;Male&amp;apos; `` or `` &amp;apos;Female&amp;apos; ``. Your job is to write a function that will recode `` &amp;apos;Female&amp;apos; `` to `` 0 ``, `` &amp;apos;Male&amp;apos; `` to `` 1 ``, and return `` np.nan `` for all entries of `` &amp;apos;sex&amp;apos; `` that are neither `` &amp;apos;Female&amp;apos; `` nor `` &amp;apos;Male&amp;apos; ``. \n",
    "Recoding variables like this is a common data cleaning task. Functions provide a mechanism for you to abstract away complex bits of code as well as reuse code. This makes your code more readable and less error prone.\n",
    "As Dan showed you in the videos, you can use the `` .apply() `` method to _apply_ a function across entire rows or columns of DataFrames. However, note that each column of a DataFrame is a pandas Series. Functions can also be applied across Series. Here, you will apply your function over the `` &amp;apos;sex&amp;apos; `` column.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Define a function named `` recode_gender() `` that has one parameter: `` gender ``.\n",
    "    \n",
    "    *   If `` gender `` equals `` &amp;apos;Male&amp;apos; ``, return `` 1 ``.\n",
    "    *   Else, if `` gender `` equals `` &amp;apos;Female&amp;apos; ``, return `` 0 ``.\n",
    "    *   If `` gender `` does not equal `` &amp;apos;Male&amp;apos; `` or `` &amp;apos;Female&amp;apos; ``, return `` np.nan ``. NumPy has been pre-imported for you.\n",
    "    \n",
    "    \n",
    "    \n",
    "*   Apply your `` recode_gender() `` function over `` tips.sex `` using the `` .apply() `` method to create a new column: `` &amp;apos;recode&amp;apos; ``. Note that when passing in a function inside the `` .apply() `` method, you don&amp;apos;t need to specify the parentheses after the function name.\n",
    "*   Hit &amp;apos;Submit Answer&amp;apos; and take note of the new `` &amp;apos;gender_recode&amp;apos; `` column in the `` tips `` DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define recode_gender()\n",
    "def recode_gender(sex_value):\n",
    "\n",
    "    # Return 1 if sex_value is 'Male'\n",
    "    if sex_value == 'Male':\n",
    "        return 1\n",
    "    \n",
    "    # Return 0 if sex_value is 'Female'    \n",
    "    elif sex_value == 'Female':\n",
    "        return 0\n",
    "    \n",
    "    # Return np.nan    \n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to the sex column\n",
    "tips['sex_recode'] = tips.sex.apply(recode_gender)\n",
    "\n",
    "# Print the first five rows of tips\n",
    "print(tips.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter  -- Exercise 10\n",
    "### Lambda functions\n",
    "You'll now be introduced to a powerful Python feature that will help you clean your data more effectively: lambda functions. Instead of using the `` def `` syntax that you used in the previous exercise, lambda functions let you make simple, one-line functions. \n",
    "\n",
    "For example, here's a function that squares a variable used in an `` .apply() `` method:\n",
    "\n",
    "`def my_square(x):\n",
    "    return x ** 2\n",
    "\n",
    "df.apply(my_square)`\n",
    "\n",
    "The equivalent code using a lambda function is:\n",
    "\n",
    "`df.apply(lambda x: x ** 2)`\n",
    "\n",
    "The lambda function takes one parameter - the variable `` x ``. The function itself just squares `` x `` and returns the result, which is whatever the one line of code evaluates to. In this way, lambda functions can make your code concise and Pythonic.\n",
    "The tips dataset has been pre-loaded into a DataFrame called `` tips ``. Your job is to clean its `` total_dollar `` column by removing the dollar sign. You'll do this using two different methods: With the `` .replace() `` method, and with regular expressions. The regular expression module `` re `` has been pre-imported.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Use the `` .replace() `` method inside a lambda function to remove the dollar sign from the `` total_dollar `` column of `` tips ``.\n",
    "    \n",
    "    *   You need to specify two arguments to the `` .replace() `` method: The string to be replaced (`` &amp;apos;$&amp;apos; ``), and the string to replace it by (`` $ ``).\n",
    "    *   Apply the lambda function over the ``total_dollar `` column of `` tips ``.\n",
    "    \n",
    "    \n",
    "    \n",
    "*   Use a regular expression to remove the dollar sign from the `` total_dollar `` column of `` tips ``.\n",
    "    \n",
    "    *   The pattern has been provided for you: It is the first argument of the `` re.findall() `` function.\n",
    "    *   Complete the rest of the lambda function and apply it over the `` total_dollar `` column of `` tips ``. Notice that because `` re.findall() `` returns a list, you have to slice it in order to access the actual value.\n",
    "    \n",
    "    \n",
    "    \n",
    "*   Hit &amp;apos;Submit Answer&amp;apos; to verify that you have removed the dollar sign from the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips=pd.read_csv('tips.csv')\n",
    "import re\n",
    "\n",
    "# Write the lambda function using replace\n",
    "tips['total_dollar_replace'] = tips.total_dollar.apply(lambda x: str(x).replace('$', ''))\n",
    "\n",
    "# Write the lambda function using regular expressions\n",
    "tips['total_dollar_re'] = tips.total_dollar.apply(lambda x: re.findall('\\d+\\.\\d+', x)[0])\n",
    "\n",
    "# Print the head of tips\n",
    "print(tips.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter  -- Exercise 12\n",
    "### Dropping duplicate data\n",
    "Duplicate data causes a variety of problems. From the point of view of performance,they use up unnecessary amounts of memory and cause unneeded calculations to be performed when processing data.In addition, they can also bias any analysis results.\n",
    "A dataset consisting of the performance of songs on the Billboard charts has been pre-loaded into a DataFrame called `` billboard ``. Check out its columns in the IPython Shell. Your job in this exercise is to subset this DataFrame and then drop all duplicate rows.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Create a new DataFrame called `` tracks `` that contains the following columns from `` billboard ``: ``year ``, `` artist ``, `` track ``, and `` time ``.\n",
    "*   Print the `` info `` of `` tracks ``. This has been done for you.\n",
    "*   Drop duplicate rows from `` tracks `` using the `` .drop_duplicates() `` method. Save the result to `` tracks_no_duplicates ``.\n",
    "*   Print the `` info `` of `` tracks_no_duplicates ``. This has been done for you, so hit &amp;apos;Submit Answer&amp;apos; to see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billboard=pd.read_csv('billboard.csv','r')\n",
    "\n",
    "# Create the new DataFrame: tracks\n",
    "tracks = billboard[['year','artist','track','time']]\n",
    "\n",
    "# Print info of tracks\n",
    "print(tracks.info())\n",
    "\n",
    "# Drop the duplicates: tracks_no_duplicates\n",
    "tracks_no_duplicates = tracks.drop_duplicates()\n",
    "\n",
    "# Print info of tracks\n",
    "print(tracks_no_duplicates.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter  -- Exercise 13\n",
    "### Filling missing data\n",
    "Here, you'll return to the `` airquality `` dataset from Chapter 2. It has been pre-loaded into the DataFrame `` airquality ``, and it hasmissing values for you to practice filling in. Explore `` airquality `` in the IPython Shell to checkout which columns have missing values.\n",
    "It&amp;apos;s rare to have a (real-world) dataset without any missing values, and it&amp;apos;simportant to deal with them because certain calculations cannot handle missing values while some calculations will, by default, skip over any missing values.\n",
    "Also, understanding how much missing data you have, and thinking about where it comes from is crucialto making unbiased interpretations of data.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Calculate the mean of the `` Ozone `` column of `` airquality `` using the `` .mean() `` method on `` airquality.Ozone ``.\n",
    "*   Use the `` .fillna() `` method to replace all the missing values in the `` Ozone `` column of `` airquality `` with the mean, `` oz_mean ``.\n",
    "*   Hit &amp;apos;Submit Answer&amp;apos; to see the result of filling in the missing values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airquality=pd.read_csv('airquality.csv')\n",
    "\n",
    "\n",
    "# Calculate the mean of the Ozone column: oz_mean\n",
    "oz_mean = airquality.Ozone.mean()\n",
    "\n",
    "# Replace all the missing values in the Ozone column with the mean\n",
    "airquality['Ozone'] = airquality.Ozone.fillna(oz_mean)\n",
    "\n",
    "\n",
    "# Print the info of airquality\n",
    "print(airquality.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter  -- Exercise 15\n",
    "### Testing your data with asserts\n",
    "Here, you'll practice writing assert statements using the Ebola dataset from previous chapters to programmatically check for missing values and to confirm that all values are positive. The dataset has been pre-loaded into a DataFrame called `` ebola ``.\n",
    "In the video, you saw Dan use the `` .all() `` method together with the `` .notnull() `` DataFrame method to check for missing values in a column. The `` .all() `` method returns `` True `` if all values are `` True ``. When used on a DataFrame, it returns a Series of Booleans - one for each column in the DataFrame. So if you are using it on a DataFrame, like in this exercise, you need to chain another `` .all() `` method so that you return only one `` True `` or `` False `` value. When using these within an assert statement, nothing will be returned if the assert statement is true: This is how you can confirm that the data you are checking are valid.\n",
    "Note: You can use `` pd.notnull(df) `` as an alternative to `` df.notnull() ``.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Write an assert statement to confirm that there are no missing values in `` ebola ``.\n",
    "    \n",
    "    *   Use the `` pd.notnull() `` function on `` ebola `` (or the `` .notnull() `` method of `` ebola ``) and chain two `` .all() `` methods (that is, `` .all().all() ``). The first `` .all() `` method will return a `` True `` or `` False `` for each column, while the second `` .all() `` method will return a single `` True `` or `` False ``.\n",
    "    \n",
    "    \n",
    "    \n",
    "*   Write an assert statement to confirm that all values in `` ebola `` are greater than or equal to `` 0 ``.\n",
    "    \n",
    "    *   Chain two `` all() `` methods to the Boolean condition (`` ebola >= 0) ``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning-data-in-python\n",
    "\n",
    "## Excercise-1\n",
    "\n",
    "\n",
    "## Exploratory analysis\n",
    "\n",
    "Whenever you obtain a new dataset, your first task should always be to do some exploratory analysis to get a better understanding of the data and diagnose it for any potential issues.\n",
    "\n",
    "The Gapminder data for the 19th century has been loaded into a DataFrame called `g1800s`. In the IPython Shell, use pandas methods such as `.head()`, `.info()`, and `.describe()`, and DataFrame attributes like `.columns` and `.shape` to explore it.\n",
    "\n",
    "Use the information that you acquire from your exploratory analysis to choose the true statement from the options provided below.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1.The DataFrame has 259 rows and 100 columns.\n",
    "\n",
    "2.The DataFrame has no missing values encoded as NaN.\n",
    "\n",
    "3.100 of the columns are of type float64 and 1 column is of type object.\n",
    "\n",
    "4.The DataFrame takes up 203.2+ KB of memory.\n",
    "\n",
    "\n",
    "### Answer-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter  -d Exercise 3\n",
    "### Visualizing your data\n",
    "Since 1800, life expectancy around the globe has been steadily going up. You would expect the Gapminder data to confirm this. \n",
    "The DataFrame `` g1800s `` has been pre-loaded. Your job in this exercise is to create a scatter plot with life expectancy in `` 1800 `` on the x-axis and life expectancy in `` 1899 `` on the y-axis.\n",
    "Here, the goal is to visually check the data for insights as well as errors. When looking at the plot, pay attention to whether the scatter plot takes the form of a diagonal line, and which points fall below or above the diagonal line. This will inform how life expectancy in 1899 changed (or did not change) compared to 1800 for different countries. If points fall on a diagonal line, it means that life expectancy remained the same!\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Import `` matplotlib.pyplot `` as `` plt ``.\n",
    "*   Use the `` .plot() `` method on `` g1800s `` with `` kind=scatter `` to create a scatter plot with `` 1800 `` on the x-axis and `` 1899 `` on the y-axis.\n",
    "*   Display the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "g1800s=pd.read_csv('gg.csv',encoding = \"ISO-8859-1\")\n",
    "# Create the scatter plot\n",
    "g1800s.plot(kind='scatter', x='1800', y='1899')\n",
    "\n",
    "# Specify axis labels\n",
    "plt.xlabel('Life Expectancy by Country in 1800')\n",
    "plt.ylabel('Life Expectancy by Country in 1899')\n",
    "\n",
    "# Specify axis limits\n",
    "plt.xlim(20, 55)\n",
    "plt.ylim(20, 55)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter  -d Exercise 4\n",
    "### Thinking about the question at hand\n",
    "Since you are given life expectancy level data by country and year, you could ask questions about how much the average life expectancy changes over each year.\n",
    "Before continuing, however, it's important to make sure that the following assumptions about the data are true:\n",
    "You can write a function that you can apply over the entire DataFrame to verify some of these assumptions. Note that spending the time to write such a script will help you when working with other datasets as well.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Define a function called `` check_null_or_valid() `` that takes in one argument: `` row_data ``.\n",
    "*   Inside the function, convert `` no_na `` to a numeric data type using `` pd.to_numeric() ``.\n",
    "*   Write an assert statement to make sure the first column (index `` 0 ``) of the `` g1800s `` DataFrame is `` Life expectancy ``.\n",
    "*   Write an assert statement to test that all the values are valid for the `` g1800s `` DataFrame. Use the `` check_null_or_valid() `` function placed inside the `` .apply() `` method for this. Note that because you&amp;apos;re applying it over the entire DataFrame, and not just one column, you&amp;apos;ll have to chain the `` .all() `` method twice, and remember that you don&amp;apos;t have to use `` () `` for functions placed inside `` .apply() ``.\n",
    "*   Write an assert statement to make sure that each country occurs only once in the data. Use the `` .value_counts() `` method on the ``Life expectancy `` column for this. Specifically, index `` 0 `` of `` .value_counts() `` will contain the most frequently occuring value. If this is equal to `` 1 `` for the `` Life expectancy `` column, then you can be certain that no country appears more than once in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_null_or_valid(row_data):\n",
    "    \"\"\"Function that takes a row of data,\n",
    "    drops all missing values,\n",
    "    and checks if all remaining values are greater than or equal to 0\n",
    "    \"\"\"\n",
    "    no_na = row_data.dropna()[1:-1]\n",
    "    numeric = pd.to_numeric(no_na)\n",
    "    ge0 = numeric >= 0\n",
    "    return ge0\n",
    "\n",
    "g1800s=pd.read_csv('gg.csv',encoding = \"ISO-8859-1\")\n",
    "# Check whether the first column is 'Life expectancy'\n",
    "assert g1800s.columns[1] == 'Life expectancy'\n",
    "\n",
    "# Check whether the values in the row are valid\n",
    "assert g1800s.iloc[:, 1:].apply(check_null_or_valid, axis=1).all().all()\n",
    "\n",
    "# Check that there is only one instance of each country\n",
    "assert g1800s['Life expectancy'].value_counts()[0] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter  -d Exercise 5\n",
    "### Assembling your data\n",
    "Here, three DataFrames have been pre-loaded: `` g1800s ``, `` g1900s ``, and `` g2000s ``. These contain the Gapminder life expectancy data for, respectively, the 19th century, the 20th century, and the 21st century.\n",
    "Your task in this exercise is to concatenate them into a single DataFrame called `` gapminder ``. This is a row-wise concatenation, similar to how you concatenated the monthly Uber datasets in Chapter 3.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Use `` pd.concat() `` to concatenate `` g1800s ``, `` g1900s ``, and `` g2000s `` into one DataFrame called `` gapminder ``. Make sure you pass DataFrames to `` pd.concat() `` in the form of a list.\n",
    "*   Print the shape and the head of the concatenated DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the DataFrames row-wise\n",
    "gapminder = pd.concat([g1800s, g1800s, g1800s])\n",
    "\n",
    "# Print the shape of gapminder\n",
    "print(gapminder.shape)\n",
    "\n",
    "# Print the head of gapminder\n",
    "print(gapminder.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter  -d Exercise 7\n",
    "### Reshaping your data\n",
    "Now that you have all the data combined into a single DataFrame, the next step is to reshape it into a _tidy_ data format.\n",
    "Currently, the gapminder DataFrame has a separate column for each year. What you want instead is a single column that contains the year, and a single column that represents the average life expectancy for each year and country. By having year in its own column, you can use it as a predictor variable in a later analysis.\n",
    "You can convert the DataFrame into the desired tidy format by _melting_ it.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Reshape `` gapminder `` by melting it. Keep `` Life expectancy `` fixed by specifying it as an argument to the `` id_vars `` parameter.\n",
    "*   Rename the three columns of the melted DataFrame to `` country ``, ``year``, and `` life_expectancy `` by passing them in as a list to `` gapminder_melt.columns ``.\n",
    "*   Print the head of the melted DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "gapminder=pd.read_csv('gapminder.csv')\n",
    "# Melt gapminder: gapminder_melt\n",
    "gapminder_melt = pd.melt(frame=gapminder, id_vars='Life expectancy')\n",
    "\n",
    "# Rename the columns\n",
    "gapminder_melt.columns = ['country', 'year', 'life_expectancy']\n",
    "\n",
    "# Print the head of gapminder_melt\n",
    "print(gapminder_melt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter  -d Exercise 8\n",
    "### Checking the data types\n",
    "Now that your data are in the proper shape, you need to ensure that the columns are of the proper data type. That is, you need to ensurethat `` country `` is of type `` object ``, `` year `` is of type `` int64 ``, and `` life_expectancy `` is of type `` float64 ``.\n",
    "The tidy DataFrame has been pre-loaded as `` gapminder ``. Explore it in the IPython Shell using the `` .info() `` method. Notice thatthe column `` year `` is of type `` object ``. This is incorrect, so you&amp;apos;ll need to use the `` pd.to_numeric() `` function to convert it to a numeric data type.\n",
    "NumPy and pandas have been pre-imported as `` np `` and `` pd ``.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Convert the `` year `` column of `` gapminder `` using `` pd.to_numeric() ``.\n",
    "*   Assert that the `` country `` column is of type `` np.object ``. This has been done for you.\n",
    "*   Assert that the `` year `` column is of type `` np.int64 ``.\n",
    "*   Assert that the `` life_expectancy `` column is of type `` np.float64 ``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the year column to numeric\n",
    "gapminder.year = pd.to_numeric(gapminder['year'], errors='coerce')\n",
    "# Test if country is of type object\n",
    "assert gapminder.country.dtypes == np.object\n",
    "\n",
    "# Test if year is of type int64\n",
    "assert gapminder.year.dtypes == np.int64\n",
    "\n",
    "# Test if life_expectancy is of type float64\n",
    "assert gapminder.life_expectancy.dtypes == np.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter  -d Exercise 9\n",
    "### Looking at country spellings\n",
    "Having tidied your DataFrame and checked the data types, your next task in the data cleaning process is to look at the `` country `` columnto see if there are any special or invalid characters you may need to deal with.\n",
    "It is reasonable to assume that country names will contain:\n",
    "To confirm that this is the case, you can leverage the power of regular expressions again. For common operations like this, Pandas has a built-in string method - `` str.contains() `` - which takes a regular expression pattern, and applies it to the Series, returning`` True `` if there is a match, and `` False `` otherwise.\n",
    "Since here you want to find the values that do _not_ match, you have to invert the boolean, which can be done using `` ~ ``. This Boolean seriescan then be used to get the Series of countries that have invalid names.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Create a Series called `` countries `` consisting of the `` &amp;apos;country&amp;apos; `` column of `` gapminder ``.\n",
    "*   Drop all duplicates from `` countries `` using the `` .drop_duplicates() `` method.\n",
    "*   Write a regular expression that tests your assumptions of what characters belong in `` countries ``:\n",
    "    \n",
    "    *   Anchor the pattern to match exactly what you want by placing a `` ^ `` in the beginning and `` $ `` in the end.\n",
    "    *   Use `` A-Za-z `` to match the set of lower and upper case letters, `` \\. `` to match periods, and `` \\s `` to match whitespace between words.\n",
    "    \n",
    "    \n",
    "    \n",
    "*   Use `` str.contains() `` to create a Boolean vector representing values that match the pattern.\n",
    "*   Invert the mask by placing a `` ~ `` before it.\n",
    "*   Subset the `` countries `` series using the `` .loc[] `` accessor and `` mask_inverse ``. Then hit &amp;apos;Submit Answer&amp;apos; to see the invalid country names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the series of countries: countries\n",
    "countries = gapminder['country']\n",
    "\n",
    "# Drop all the duplicates from countries\n",
    "countries = countries.drop_duplicates()\n",
    "\n",
    "# Write the regular expression: pattern\n",
    "pattern = '^[A-Za-z\\.\\s]*$'\n",
    "\n",
    "# Create the Boolean vector: mask\n",
    "mask = countries.str.contains(pattern)\n",
    "\n",
    "# Invert the mask: mask_inverse\n",
    "mask_inverse = ~countries.str.contains(pattern)\n",
    "\n",
    "# Subset countries using mask_inverse: invalid_countries\n",
    "invalid_countries = countries.loc[mask_inverse]\n",
    "\n",
    "# Print invalid_countries\n",
    "print(invalid_countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Chapter  -d Exercise 10\n",
    "### More data cleaning and processing\n",
    "It&amp;apos;s now time to deal with the missing data. There are several strategies for this: You can drop them, fill them in using the mean of the column or row that the missing value is in (also known as [imputation](https://en.wikipedia.org/wiki/Imputation_(statistics))), or, if you are dealing with time series data, use a forward fill or backward fill, in which you replace missing values in a column with the most recent known value in the column. See [pandas Foundations](https://www.datacamp.com/courses/pandas-foundations) for more on forward fill and backward fill.\n",
    "In general, it is not the best idea to drop missing values, because in doing so you may end up throwing away useful information. In this data, the missing values refer to years where no estimate for life expectancy is available for a given country. You could fill in, or guess what these life expectancies could be by looking at the average life expectancies for other countries in that year, for example. Whichever strategy you go with, it is important to carefully consider all options and understand how they will affect your data.\n",
    "In this exercise, you&amp;apos;ll practice dropping missing values. Your job is to drop all the rows that have `` NaN `` in the `` life_expectancy `` column. Before doing so, it would be valuable to use assert statements to confirm that `` year `` and `` country `` do not have any missing values.\n",
    "Begin by printing the shape of `` gapminder `` in the IPython Shell prior to dropping the missing values. Complete the exercise to find out what its shape will be _after_ dropping the missing values!\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Assert that `` country `` and `` year `` do not contain any missing values. The first assert statement has been written for you. Note the chaining of the `` .all() `` method to `` pd.notnull() `` to confirm that _all_ values in the column are not null.\n",
    "*   Drop the rows in the data where _any_ observation in `` life_expectancy `` is missing. As you confirmed that `` country ``and `` year `` don&amp;apos;t have missing values, you can use the `` .dropna() `` method on the _entire_ `` gapminder `` DataFrame, because any missing values would have to be in the `` life_expectancy `` column. The `` .dropna() `` method has the default keyword arguments `` axis=0 `` and `` how=&amp;apos;any&amp;apos; ``, which specify that _rows_ with _any_ missing values should be dropped.\n",
    "*   Print the shape of `` gapminder ``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that country does not contain any missing values\n",
    "assert pd.notnull(gapminder.country).all()\n",
    "\n",
    "# Assert that year does not contain any missing values\n",
    "assert pd.notnull(gapminder.year).all()\n",
    "\n",
    "# Drop the missing values\n",
    "gapminder = gapminder.dropna()\n",
    "\n",
    "# Print the shape of gapminder\n",
    "print(gapminder.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter  -d Exercise 11\n",
    "### Wrapping up\n",
    "Now that you have a clean and tidy dataset, you can do a bit of visualization and aggregation.In this exercise, you&amp;apos;ll begin by creating a histogram of the `` life_expectancy `` column.You should not get any values under 0 and you should see something reasonable on the higher end of the `` life_expectancy `` age range.\n",
    "Your next task is to investigate how average life expectancy changed over the years.To do this, you need to subset the data by each year, get the `` life_expectancy `` column from each subset, and take an average of the values.You can achieve this using the [`` .groupby() `` method](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html). This`` .groupby() `` method is covered in greater depth in [Manipulating DataFrames with pandas](https://www.datacamp.com/courses/manipulating-dataframes-with-pandas).\n",
    "Finally, you can save your tidy and summarized DataFrame to a file using the `` .to_csv() `` method.\n",
    "matplotlib.pyplot and pandas have been pre-imported as `` plt `` and `` pd ``. Go for it!\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "\n",
    "*   Create a histogram of the `` life_expectancy `` column using the `` .plot() `` method of `` gapminder ``. Specify `` kind=&amp;apos;hist&amp;apos; ``.\n",
    "*   Group `` gapminder `` by `` &amp;apos;year&amp;apos; `` and aggregate `` &amp;apos;life_expectancy&amp;apos; `` by the mean. To do this:\n",
    "    \n",
    "    *   Use the `` .groupby() `` method on `` gapminder `` with `` &amp;apos;year&amp;apos; `` as the argument. Then select `` &amp;apos;life_expectancy&amp;apos; `` and chain the `` .mean() `` method to it.\n",
    "    \n",
    "    \n",
    "    \n",
    "*   Print the head and tail of `` gapminder_agg ``. This has been done for you.\n",
    "*   Create a line plot of average life expectancy per year by using the `` .plot() `` method (without any arguments in plot) on `` gapminder_agg ``.\n",
    "*   Save `` gapminder `` and `` gapminder_agg `` to csv files called `` &amp;apos;gapminder.csv&amp;apos; `` and `` &amp;apos;gapminder_agg.csv&amp;apos; ``, respectively, using the `` .to_csv() `` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add first subplot\n",
    "plt.subplot(2, 1, 1) \n",
    "\n",
    "# Create a histogram of life_expectancy\n",
    "gapminder.life_expectancy.plot(kind='hist')\n",
    "\n",
    "# Group gapminder: gapminder_agg\n",
    "gapminder_agg = gapminder.groupby('year')['life_expectancy'].mean()\n",
    "\n",
    "# Print the head of gapminder_agg\n",
    "print(gapminder_agg.head())\n",
    "\n",
    "# Print the tail of gapminder_agg\n",
    "print(gapminder_agg.tail())\n",
    "\n",
    "# Add second subplot\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Create a line plot of life expectancy per year\n",
    "gapminder_agg.plot()\n",
    "\n",
    "# Add title and specify axis labels\n",
    "plt.title('Life expectancy over the years')\n",
    "plt.ylabel('Life expectancy')\n",
    "plt.xlabel('Year')\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save both DataFrames to csv files\n",
    "gapminder.to_csv('gapminder.csv')\n",
    "gapminder_agg.to_csv('gapminder_agg.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
